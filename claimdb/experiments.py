"""Here, we evaluate different LLM agents on our benchmark"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/07_experiments.ipynb.

# %% auto #0
__all__ = ['toolbox_client', 'BASE_PROMPT', 'FACT_CHECKER_PROMPT', 'EX1', 'EX2', 'EX3', 'FACT_CHECKER_PROMPT_3SHOT',
           'bird_id_to_example_dict', 'db_names', 'tool_cache', 'model', 'batch_size', 'results_path', 'benchmark',
           'description', 'ClaimVerdict', 'run_result_to_dict', 'return_coroutines', 'run_tests']

# %% ../nbs/07_experiments.ipynb #b04205a3
from toolbox_core import ToolboxSyncClient
from .configuration import *

# %% ../nbs/07_experiments.ipynb #8c9c7ae0
from pydantic import BaseModel, Field
from typing import Literal
from agents import Runner, Agent, function_tool
import json

# %% ../nbs/07_experiments.ipynb #eac6b51f
from .transformation import claim_collection_json_to_parsed

# %% ../nbs/07_experiments.ipynb #c08449fe
toolbox_client = ToolboxSyncClient("http://127.0.0.1:5000")

# %% ../nbs/07_experiments.ipynb #d9fc38ae
def description(pydantic_model):
    "Print the field descriptions of a Pydantic model"
    for name, field in pydantic_model.model_fields.items():
        print(f"{name}: {field.description}\n")

# %% ../nbs/07_experiments.ipynb #0885604a
class ClaimVerdict(BaseModel):
    verdict: Literal["ENTAILED", "CONTRADICTED", "NOT ENOUGH INFO"] = Field(
        ...,
        description="Whether the claim is supported, contradicted, or undecidable from the database."
    )
    justification: str = Field(
        ...,
        description="Brief justification (1-2 sentences) of the verdict."
    )

# %% ../nbs/07_experiments.ipynb #0fc8e332
BASE_PROMPT = f"""
You are a fact-checking assistant operating over structured data. You will be given a natural-language claim and optional external information. You will have access to a SQLite database and may execute arbitrary SQL queries over it using specialized tools.

Your task is to determine whether the claim is "ENTAILED", "CONTRADICTED", or "NOT ENOUGH INFO" based on evidence you obtain from the database. The labels are defined as follows:

- ENTAILED: The claim is supported by the database.
- CONTRADICTED: The claim is refuted by the database.
- NOT ENOUGH INFO: The database does not provide sufficient evidence to decide.

Use the available tools to query the database and gather evidence before making a decision. Do not ask the user for clarification or additional information.

You should always start by querying the database for the schema (tables and columns).
"""

FACT_CHECKER_PROMPT = BASE_PROMPT + f"""
Your answer should be in JSON format, adhering to the following schema:
{json.dumps(ClaimVerdict.model_json_schema(), indent=2)}
"""


# %% ../nbs/07_experiments.ipynb #4237a000
EX1 = """
Output Example 1:
{
    "verdict": "ENTAILED",
    "justification": "The database shows that the population of France is 67 million, which supports the claim."
}
"""

EX2 = """
Output Example 2:
{
    "verdict": "CONTRADICTED",
    "justification": "The database indicates that the capital of Germany is Berlin, contradicting the claim."
}
"""

EX3 = """
Output Example 3:
{
    "verdict": "NOT ENOUGH INFO",
    "justification": "The database does not contain any information about the population of Sacramento."
}
"""

# %% ../nbs/07_experiments.ipynb #75194875
FACT_CHECKER_PROMPT_3SHOT = FACT_CHECKER_PROMPT + "\n" + EX1 + EX2 + EX3

# %% ../nbs/07_experiments.ipynb #e0707dea
def run_result_to_dict(result, ollama=False) -> dict:
    """Convert an Agent's RunResult to a dictionary."""
    info_dict = {}

    if isinstance(result, Exception):
        return {
            'verdict': "",
            'error': str(result),
            'justification': "",
            'model_name': "",
            'model_settings': "",
            'usage': [],
            'to_input_list': []
        }

    # 1. Final output
    info_dict['verdict'] = result.final_output.verdict
    info_dict['justification'] = result.final_output.justification
    info_dict['final_output'] = str(result.final_output)

    # 2. Model Settings
    info_dict['model_name'] = result._last_agent.model
    if ollama: info_dict['model_name'] = info_dict['model_name'].model
    info_dict['model_settings'] = result._last_agent.model_settings.to_json_dict()

    # 3. All Requests Costs (the total is the sum)
    usage = []
    for request_usage in result.context_wrapper.usage.request_usage_entries:
        cached_input_tokens = request_usage.input_tokens_details.cached_tokens
        regular_input_tokens = request_usage.input_tokens - cached_input_tokens
        output_tokens = request_usage.output_tokens

        usage.append(
            {
                "regular_input_tokens": regular_input_tokens,
                "cached_input_tokens": cached_input_tokens,
                "output_tokens": output_tokens,
            }
        )
    info_dict['usage'] = usage

    # 4. The complete Agentic Pipeline
    info_dict['to_input_list'] = result.to_input_list()

    return info_dict

# %% ../nbs/07_experiments.ipynb #f1fe1959
import asyncio
import random

# %% ../nbs/07_experiments.ipynb #b7b46c16
bird_id_to_example_dict = dict()

with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:
    for line in f:
        parsed = json.loads(line)
        bird_id = parsed['bird_id']
        bird_id_to_example_dict[bird_id] = parsed
    
len(bird_id_to_example_dict)

# %% ../nbs/07_experiments.ipynb #624d70da
db_names = set(v['db_id'] for v in bird_id_to_example_dict.values())

# %% ../nbs/07_experiments.ipynb #151dcb24
_tool_cache = dict()
tool_cache = dict()

for db_name in db_names:
    tool = toolbox_client.load_tool(f"{db_name}_execute_sql")
    _tool_cache[db_name] = tool
    tool_cache[db_name] = function_tool(tool)

# %% ../nbs/07_experiments.ipynb #531d6747
def return_coroutines(test_claims, model):
    cors = []
    claim_ids = []

    for claim in test_claims:

        tool = tool_cache[claim['db_name']]

        fact_checker_agent = Agent(
            name="Fact-Checker",
            instructions=FACT_CHECKER_PROMPT_3SHOT,
            model=model,
            tools=[tool],
            output_type=ClaimVerdict,
        )

        inp = f"Claim: {claim['claim']}\nExtra Information: {claim['extra_info']}"

        cors.append(Runner.run(fact_checker_agent, inp, max_turns=20))
        
        claim_ids.append(claim['claim_id'])
    
    return cors, claim_ids

# %% ../nbs/07_experiments.ipynb #9199fcf5
#TODO: tool returns exception without messing up the dependancies on 07b.

#model = "gpt-5-mini"
#model = "gpt-4.1-nano"
model = "gpt-5-nano"
model = "gpt-4o-mini"

batch_size = 100

results_path = config.experiments_dir_pub / f"{model}.jsonl"
results_path.touch()

# %% ../nbs/07_experiments.ipynb #e13ddfab
import asyncio

# %% ../nbs/07_experiments.ipynb #f1b1532c
with open(results_path, 'r') as f:
    already_tested = [json.loads(line)['claim_id'] for line in f]

benchmark = []
with open(config.final_benchmark_dir / 'test-public.jsonl') as f:
    for line in f: 
        parsed_claim = json.loads(line)
        if parsed_claim['claim_id'] in already_tested: continue
        benchmark.append(parsed_claim)

# %% ../nbs/07_experiments.ipynb #8028fa89
async def run_tests():

    for i in range(0, len(benchmark), batch_size):
        test_claims = benchmark[i:i+batch_size]

        cors, claim_ids = return_coroutines(test_claims, model)

        results = await asyncio.gather(*cors, return_exceptions=True)

        for claim_id, res in zip(claim_ids, results):
            results_dict = {'claim_id': claim_id} | run_result_to_dict(res)
            results_path.open('a').write(json.dumps(results_dict) + '\n')

# %% ../nbs/07_experiments.ipynb #9d2a1fbe
try: from nbdev.imports import IN_NOTEBOOK
except: IN_NOTEBOOK=False

# %% ../nbs/07_experiments.ipynb #aa4af0de
if __name__ == "__main__" and not IN_NOTEBOOK:
    print(f"#Exps Left: {len(benchmark)}")
    print(model)
    asyncio.run(run_tests())

"""Here, we transform the benchmark into the Fact-Checking benchmark"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_transformation.ipynb.

# %% auto #0
__all__ = ['ENTAILED_INSTR', 'NO_INFO_INSTR', 'ContradictedClaim', 'ContradictedClaimCollection', 'EntailedClaim',
           'EntailedClaimCollection', 'NoInfoClaim', 'NoInfoClaimCollection', 'construct_payload',
           'claim_collection_json_to_parsed']

# %% ../nbs/04_transformation.ipynb #ae0e678b
import json
from .configuration import *
from .preprocess_bird import *
from pydantic import BaseModel, Field
from openai import OpenAI

# %% ../nbs/04_transformation.ipynb #e0971ac9
class ContradictedClaim(BaseModel):
    contradicted_claim: str = Field(
        ...,
        description=(
            "A contradicted claim."
        )
    )

class ContradictedClaimCollection(BaseModel):
    collection: list[ContradictedClaim]

# %% ../nbs/04_transformation.ipynb #461111fd
class EntailedClaim(BaseModel):
    entailed_claim: str = Field(
        ...,
        description=(
            "An entailed claim."
        )
    )

class EntailedClaimCollection(BaseModel):
    collection: list[EntailedClaim]

# %% ../nbs/04_transformation.ipynb #076a69e5
ENTAILED_INSTR = """
## Role
You are an **honest** spokesperson **in a controlled evaluation setting**.

## Task
Given the following inputs:
- A question
- Its correct answer
- The data domain
- Optional external knowledge (clarifications)

Your task is to produce natural language claims that are consistent with and supported by the provided answer. In other words, any reader who knows the correct answer would judge your claim to be true.

## Requirements
- Each claim must be self-contained and must not use opaque references to earlier context (e.g., "the answer," "the question," "the earlier claim", etc.). Instead, any needed context should be stated explicitly within each claim.
- Each claim must follow from or be fully supported by the answer, directly or indirectly.
- Do not restate or explain the external knowledge; assume it is already known to the reader.
- Produce between 1 and 3 claims.

## Example

### Input
{
  "question": "Which three districts recorded the highest graduation rates in 2022?",
  "answer": [
    {
      "DistrictName": "Redwood Coast Unified",
      "GradRate": 0.97
    },
    {
      "DistrictName": "Sierra Vista Union",
      "GradRate": 0.96
    },
    {
      "DistrictName": "Mission Creek Unified",
      "GradRate": 0.95
    }
  ],
  "domain": "California Schools",
  "external-knowledge": "GradRate = Number of graduates / Total number of eligible seniors"
}

### Output
Redwood Coast Unified led California's graduation rankings in 2022 with a 97% rate.

In 2022, California's strongest graduation results came from Redwood Coast Unified, which saw 97% of its eligible seniors finish high school. Sierra Vista Union and Mission Creek Unified followed closely, with graduation rates of 96% and 95%, respectively.

Mission Creek Unified achieved a graduation rate of 95% in 2022, placing it among California's top three districts. It ranked just behind Redwood Coast Unified and Sierra Vista Union. The rate indicates the percentage of eligible seniors who graduated.
"""

#- Additional claims must differ in factual content, not just wording. Avoid simple paraphrases.

# %% ../nbs/04_transformation.ipynb #d52b62a8
from typing import Literal

# %% ../nbs/04_transformation.ipynb #84b9ed8a
class NoInfoClaim(BaseModel):
    no_info_claim: str = Field(
        ...,
        description="A NOT ENOUGH INFO claim."
    )
    category: Literal["Out-of-Schema", "Subjective", "Counterfactual"] = Field(
        ...,
        description="The category of the NOT ENOUGH INFO claim."
    )

class NoInfoClaimCollection(BaseModel):
    collection: list[NoInfoClaim]

# %% ../nbs/04_transformation.ipynb #656812ef
NO_INFO_INSTR = """
## Role
You are a neutral spokespearson **in a controlled evaluation setting**.

## Task
Given the following inputs:
- A question
- Its correct answer
- The data domain
- The schema of the database
- Optional external knowledge (clarifications)

Your task is to produce natural language claims whose truth **cannot** be determined from the database or the given Q/A. That is, even with full access to both the database and the correct answer, these claims cannot be definitively verified or falsified.

## Requirements
- Each claim must be self-contained and must not use opaque references to earlier context (e.g., "the answer," "the question," "the earlier claim", etc.). Instead, any needed context should be stated explicitly within each claim.
- Each claim must *not* be entailed or contradicted by the answer, directly or indirectly.
- Each claim must fall into at least one of these categories:
  1. **Out-of-schema** — involves concepts the database doesn't store or represent anywhere in its schema.
  2. **Subjective/evaluative** — expresses opinions or judgments that cannot be objectively verified.
  3. **Counterfactual/hypothetical** — describes an imagined or "what if" situation that is not reflected in the actual data.
- Produce between 1 and 5 claims.
- Do not restate or explain the external knowledge; assume it is already known to the reader.
"""

# - Each claim must be self-contained. For example, they must not contain meta-information like "the answer", "the question", etc.


# %% ../nbs/04_transformation.ipynb #e31ca7c6
from openai.lib._parsing._responses import type_to_text_format_param, parse_response

# %% ../nbs/04_transformation.ipynb #3010cb95
from typing import Any

# %% ../nbs/04_transformation.ipynb #cec3adf6
def construct_payload(request_id: str, # unique (for this batch) request id
                      model: str, # model name
                      instr: str, # the prompt w/ instructions
                      inp: str, # the input
                      format_type: Any # the expected output format type (e.g., NoInfoClaimCollection)
                      ) -> dict: # returns the payload dict
    """ Construct the payload for the Batch API request. """

    payload = {
        "custom_id": request_id,
        "method": "POST",
        "url": "/v1/responses",
        "body": {
            "model": model,
            "input": [
                {"role": "developer", "content": instr},
                {"role": "user", "content": inp}
            ],
            "text": {"format": type_to_text_format_param(format_type)}
        }
    }

    return payload

# %% ../nbs/04_transformation.ipynb #c1f45f98
from openai.types.responses import Response

# %% ../nbs/04_transformation.ipynb #1cbdf6db
def claim_collection_json_to_parsed(claim_collection: dict  # A specific claim collection (e.g., all "ENTAILED" claims for the specific BIRD Q/A)
                         ):
    """ Parses a claim collection to the OpenAI format with the matched classes.
    Each such claim collection is about a specific BIRD Q/A and a specific label (e.g., ENTAILED).
    Returns:
        bird_id: The BIRD Q/A ID.
        label: The label of the claim collection (all claims in the collection are this).
        claim_collection_parsed: The parsed claim collection.
    """
    custom_id = claim_collection['custom_id']
    bird_id, _, label = custom_id.split("-")

    bird_id = int(bird_id)

    if label == "Entailed":
        label = "ENTAILED"
        output_format = EntailedClaimCollection
    if label == "Contradicted":
        label = "CONTRADICTED"
        output_format = ContradictedClaimCollection
    if label == "NoInfo":
        label = "NOT ENOUGH INFO"
        output_format = NoInfoClaimCollection

    claim_collection_parsed = parse_response(
        response=Response.model_validate(claim_collection['response']['body']),
        text_format=output_format,
        input_tools=[]
    )

    return bird_id, label, claim_collection_parsed

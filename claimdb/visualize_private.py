"""Here, we see experiment results!"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08a_visualize.ipynb.

# %% auto #0
__all__ = ['all_claims', 'claim_map', 'load_df']

# %% ../nbs/08a_visualize.ipynb #3f6c7f98
import json
from .configuration import config

# %% ../nbs/08a_visualize.ipynb #1a7e77ff
with open(config.final_benchmark_dir / 'test-public.jsonl', "r") as f:
    pub_test = [json.loads(line) for line in f]
    pub_ids = [item['claim_id'] for item in pub_test]

with open(config.final_benchmark_dir / 'train.jsonl', "r") as f:
    train = [json.loads(line) for line in f]

# this will fail -- not available publicly
with open(config.final_benchmark_dir / 'test-private-with-labels.jsonl', "r") as f:
    priv_test = [json.loads(line) for line in f]
    priv_ids = [item['claim_id'] for item in priv_test]

all_claims = pub_test + train + priv_test

claim_map = {item['claim_id']: item for item in all_claims}

# %% ../nbs/08a_visualize.ipynb #c055baf0
def load_df(model_name, split):
    predicted_results = []

    ids = []
    if split == 'private':
        path = config.experiments_dir_priv / f"{model_name}.jsonl"
        ids = priv_ids
    if split == 'public':
        path = config.experiments_dir_pub / f"{model_name}.jsonl"
        ids = pub_ids
    
    ids = list(ids)

    if not os.path.exists(path): return None
    
    with open(path, "r") as f:

        for line in f:

            log = json.loads(line)
            claim_id = log['claim_id']
            if claim_id not in ids: 
                continue
            else:
                ids.remove(claim_id)

            verdict = log['verdict']

            category = claim_map[claim_id].get('category', None)
            ground_truth = claim_map[claim_id]['label']
            db_name = claim_map[claim_id]['db_name']

            # Tool Calls
            num_tool_calls = None
            tokens = 0

            if "model_settings" in log and not "error" in log:
                num_tool_calls = 0
                for item in log['to_input_list']:
                    if item.get('type', None) == 'function_call':
                        num_tool_calls += 1

            if "model_settings" in log and 'usage' in log:   # Means we are in OpenAI's SDK
                usage_logs = log['usage']
                tokens = 0
                for usage_dict in usage_logs:
                    tokens += sum(usage_dict.values())

                #print(f"{model_name} : {tokens} total tokens")

            if "model_settings" not in log and not "error" in log:  # Means we are in Pydantic's Agents
                if log.get("error", None): 
                    num_tool_calls = None
                    continue
                num_tool_calls = 0
                for message in log.get('all_messages', []):
                    # Check if this is a response message
                    if message.get('kind') == 'response':
                        # Look through the parts for tool calls
                        for part in message.get('parts', []):
                            if part.get('part_kind') == 'tool-call':
                                num_tool_calls += 1
                
            if "model_settings" not in log: # Means we are in Pydantic's Agents
                tokens = 0
                for msg in log.get('all_messages', []):
                    if 'usage' not in msg:
                        continue
                    tokens += sum([t for t in msg['usage'].values() if isinstance(t, int)])

            # NOTE: gpt-5-nano on private does not have "model_settings" but has "usage"
            if model_name == 'gpt-5-nano':
                usage_logs = log['usage']
                tokens = 0
                for usage_dict in usage_logs:
                    tokens += sum(usage_dict.values())
                
                #print(f"{model_name} : {tokens} total tokens")

            entry = {
                'claim_id': claim_id,
                'verdict': verdict,
                'ground_truth': ground_truth,
                'category': category,
                'db_name': db_name,
                'tool_calls': num_tool_calls,
                'tokens': tokens
            }

            predicted_results.append(entry)
        
        for missing_id in ids:
            claim = claim_map[missing_id]
            entry = {
                'claim_id': missing_id,
                'verdict': 'MISSING',
                'ground_truth': claim['label'],
                'category': claim.get('category', None),
                'db_name': claim['db_name'],
                'tool_calls': None,
                'tokens': 0
            }
            predicted_results.append(entry)
    
    df = pd.DataFrame(predicted_results)
    df['correct'] = df['verdict'] == df['ground_truth']

    return df

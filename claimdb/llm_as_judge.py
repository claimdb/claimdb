"""In this notebook, we will filter out data using LLMs"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05a_filter_judges.ipynb.

# %% auto #0
__all__ = ['regular_system_prompt', 'nei_system_prompt', 'endpoint', 'client', 'bird_id_example_map', 'all_claims_dict',
           'ce_human_annot_claim_ids', 'nei_human_annot_claim_ids', 'ce_test', 'nei_test', 'ce_train', 'nei_train',
           'train', 'test', 'all_test', 'batch_size', 'sleep_time', 'deployment_name', 'path', 'claim_ids',
           'experiment_ids', 'ClaimQuality', 'NEIClaimQuality', 'return_coroutines', 'construct_user_prompt',
           'parse_judge_response', 'send_to_judge', 'main']

# %% ../nbs/05a_filter_judges.ipynb #0ccd698d
from .configuration import *
import json
from pydantic import BaseModel, Field
from typing import Literal
from .preprocess_bird import *
from .configuration import *
import time

# %% ../nbs/05a_filter_judges.ipynb #d8f39476
class ClaimQuality(BaseModel):
    label_correct: Literal["yes", "no"] = Field(
        ..., 
        description="Is the assigned label of the claim (ENTAILED/CONTRADICTED) correct given the gold information? Answer \"yes\" if the label of the claim follows from the gold information; \"no\" otherwise. If you are unsure, answer \"no\"."
    )

    free_of_meta_references: Literal["yes", "no"] = Field(
        ...,
        description="Does the claim avoid meta-references to the question, answer, or prior text (e.g., \"this question\", \"the answer above\", \"as mentioned earlier\")? Answer \"yes\" if it is completely free of meta-references; \"no\" otherwise. References to provided external knowledge do not count as meta-references."
    )
    
    reasoning: str = Field(
        ...,
        description="Brief explanation (1-2 sentences) justifying your evaluation (especially for \"label_correct\")."
    )
    def __eq__(self, other):
        """Compare all fields except reasoning."""
        return (
            self.label_correct == other.label_correct
            and self.free_of_meta_references == other.free_of_meta_references
        )

# %% ../nbs/05a_filter_judges.ipynb #959194e6
class NEIClaimQuality(BaseModel):
    label_correct: Literal["yes", "no"] = Field(
        ..., 
        description="Is the assigned label of the claim (NOT ENOUGH INFO) correct given the gold information? Answer \"yes\" if the label of the claim follows from the gold information; \"no\" otherwise. If you are unsure, answer \"no\"."
    )

    free_of_meta_references: Literal["yes", "no"] = Field(
        ...,
        description="Does the claim avoid meta-references to the question, answer, or prior text (e.g., \"this question\", \"the answer above\", \"as mentioned earlier\")? Answer \"yes\" if it is completely free of meta-references; \"no\" otherwise. References to provided external knowledge do not count as meta-references."
    )

    category_correct: Literal["yes", "no"] = Field(
        ..., 
        description="Is the assigned category of the claim (OUT-OF-SCHEMA/COUNTERFACTUAL/SUBJECTIVE) correct given the gold information? Answer \"yes\" if the category of the claim follows from the gold information; \"no\" otherwise."
    )

    schema_leakage: Literal["yes", "no"] = Field(
        ...,
        description="Does the claim expose database schema details or technical artifacts? Answer \"yes\" if it exposes schema details (e.g., table names, column names, etc.); \"no\" if it does not."
    )
    
    reasoning: str = Field(
        ...,
        description="Brief explanation (1-2 sentences) justifying your evaluation (especially for \"label_correct\" and \"category_correct\")."
    )

    def __eq__(self, other):
        """Compare all fields except reasoning."""
        return (
            self.label_correct == other.label_correct
            and self.free_of_meta_references == other.free_of_meta_references
            and self.category_correct == other.category_correct
            and self.schema_leakage == other.schema_leakage
        )

# %% ../nbs/05a_filter_judges.ipynb #e87bccd8
regular_system_prompt =f"""
Your task is to evaluate a natural-language claim across two criteria. You will be given a gold context composed of a question, its answer, the domain, and optional external knowledge. Treat the gold context as the authoritative ground truth.

You will be given a claim labeled as either ENTAILED (supported by the gold context) or CONTRADICTED (refuted by the gold context). Using this gold information, assess whether the claim is correctly labeled, and whether it is free of meta-references. More detailed instructions for the two evaluation criteria are provided along with the JSON schema below.

Your answer should be in JSON format, adhering to the following schema:
{json.dumps(ClaimQuality.model_json_schema(), indent=2)}
"""

# %% ../nbs/05a_filter_judges.ipynb #b3e981d6
nei_system_prompt =f"""
Your task is to evaluate a natural-language claim across several criteria. You will be given a gold context composed of a question, its answer, the domain, optional external knowledge, and the complete database schema underlying the gold context. Treat the gold context as the authoritative ground truth.

You will be given a claim with an assigned NOT ENOUGH INFO (NEI) label, meaning that its truth cannot be determined from the gold context, even with full access to the database. The claim is also assigned an NEI category: OUT-OF-SCHEMA (depends on information not stored in the database), SUBJECTIVE (expresses opinions or judgments), or COUNTERFACTUAL (describes hypothetical scenarios). Using the gold context, assess whether the NEI label and category are correct, whether the claim is free of meta-references, and whether it leaks schema details of the database. More detailed descriptions for each evaluation criterion are provided in the JSON schema below.

Your answer should be in JSON format, adhering to the following schema:
{json.dumps(NEIClaimQuality.model_json_schema(), indent=2)}
"""

# %% ../nbs/05a_filter_judges.ipynb #ae1a5d1f
from openai import AsyncOpenAI
from .configuration import *
import asyncio
from .preprocess_bird import *

import random
import json
random.seed(42)

# %% ../nbs/05a_filter_judges.ipynb #842184ba
endpoint = "https://agenticllms.cognitiveservices.azure.com/openai/v1/"

client = AsyncOpenAI(
    base_url=f"{endpoint}",
    api_key=config.openai_api_key_from_azure
)

# %% ../nbs/05a_filter_judges.ipynb #80dad5ef
bird_id_example_map = dict()
with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:
    for line in f:
        example = json.loads(line)
        bird_id_example_map[example['bird_id']] = example

all_claims_dict = dict()
with open(config.output_data_dir / 'all_claims.jsonl', 'r') as f:
    for line in f:
        parsed = json.loads(line)
        claim_id = parsed['claim_id']
        all_claims_dict[claim_id] = parsed

# %% ../nbs/05a_filter_judges.ipynb #84c87da9
def return_coroutines(user_prompts, system_prompts, deployment_name):
    cors = []

    for user_prompt, system_prompt in zip(user_prompts, system_prompts):

        #print(system_prompt + "\n")
        #print(user_prompt + "\n")
        #print("-" * 50 + "\n\n\n\n\n\n")
        
        cor = client.chat.completions.create(
            model=deployment_name,
            temperature=0.0,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={'type': 'json_object'}
        )

        cors.append(cor)
    
    return cors

# %% ../nbs/05a_filter_judges.ipynb #cf7d91d9
def construct_user_prompt(claim, bird_example):

    if claim['label'] == 'NOT ENOUGH INFO':
        user_prompt = f"""
Golden Information:
{format_for_llm(prepare_bird_example(bird_example, with_schema=True))}

Claim to Evaluate: {claim['claim']}
Assigned Label: {claim['label']}
Assigned Category: {claim['category']}
"""  
    else:
        user_prompt = f""" 
Golden Information:
{format_for_llm(prepare_bird_example(bird_example))}

Claim to Evaluate: {claim['claim']}
Assigned Label: {claim['label']}
"""
    return user_prompt 

# %% ../nbs/05a_filter_judges.ipynb #bb0cce4e
def parse_judge_response(claim_id, response):
    claim = all_claims_dict[claim_id]
    label = claim['label']
    
    if label == 'NOT ENOUGH INFO':
        parsed = NEIClaimQuality.model_validate_json(response.choices[0].message.content)
    else:
        parsed = ClaimQuality.model_validate_json(response.choices[0].message.content)
    
    return parsed

# %% ../nbs/05a_filter_judges.ipynb #2810fd9e
async def send_to_judge(claim_queue, deployment_name, batch_size=50, sleep_time=60):

    claim_id_to_result = dict()

    count_api_calls = 0
    
    total_claims = len(claim_queue)

    while claim_queue:
        print(f"Remaining claims to process: {len(claim_queue)} / {total_claims}")

        claim_ids, user_prompts, system_prompts = [], [], []

        # Handle case when queue has fewer items than batch_size
        current_batch_size = min(batch_size, len(claim_queue))

        for _ in range(current_batch_size):
            claim_id = claim_queue.pop()
            claim = all_claims_dict[claim_id]
            count_api_calls += 1

            bird_example = bird_id_example_map[claim['bird_id']]

            user_prompt = construct_user_prompt(claim, bird_example)

            if claim['label'] == 'NOT ENOUGH INFO': system_prompt = nei_system_prompt
            else: system_prompt = regular_system_prompt

            #print(f"id: {claim_id}")
            user_prompts.append(user_prompt)
            claim_ids.append(claim_id)
            system_prompts.append(system_prompt)
        
        cors = return_coroutines(
            user_prompts=user_prompts, 
            system_prompts=system_prompts, 
            deployment_name=deployment_name
        )

        results = await asyncio.gather(*cors, return_exceptions=True)

        time.sleep(sleep_time)

        bad_content_count = 0
        for claim_id, res in zip(claim_ids, results):

            if isinstance(res, Exception):
                print(f"Exception for claim_id {claim_id}: {str(res)}")
                claim_queue.append(claim_id)
                continue
            try:
                _ = parse_judge_response(claim_id, res)
            except Exception as e:
                bad_content_count += 1
                #print(res.choices[0].message.content)
                claim_queue.append(claim_id)
                continue

            # Store the result (raw response object)
            claim_id_to_result[claim_id] = res
            with open(config.judges_dir / f"{deployment_name}.jsonl", "a") as f:
                entry = {"claim_id": claim_id, "response": res.to_dict()}
                f.write(json.dumps(entry) + "\n")
        print(f"Bad content count in this batch: {bad_content_count} / {current_batch_size}")
    
    print(f"Total API calls made: {count_api_calls}")
    return claim_id_to_result
    

# %% ../nbs/05a_filter_judges.ipynb #3a94075d
with open(config.judges_dir / "human_annotations.jsonl", "r") as f:
    human_annot_claims = [json.loads(line) for line in f.readlines()]

random.seed(42)
random.shuffle(human_annot_claims)

# %% ../nbs/05a_filter_judges.ipynb #c6fe6d13
ce_human_annot_claim_ids = [
    claim['claim_id'] for claim in human_annot_claims
    if claim['label'] != 'NOT ENOUGH INFO'
]

nei_human_annot_claim_ids = [
    claim['claim_id'] for claim in human_annot_claims
    if claim['label'] == 'NOT ENOUGH INFO'
]

# %% ../nbs/05a_filter_judges.ipynb #075fd946
ce_test = ce_human_annot_claim_ids[:75]
nei_test = nei_human_annot_claim_ids[:75]

ce_train = ce_human_annot_claim_ids[75:]
nei_train = nei_human_annot_claim_ids[75:]

train = ce_train + nei_train
test = ce_test + nei_test

all_test = ce_train + nei_train + ce_test + nei_test

# %% ../nbs/05a_filter_judges.ipynb #ee15244e
batch_size = 250
sleep_time = 61

#deployment_name = "Phi-4"
deployment_name = "grok-3-mini"
#eployment_name = "mistral-small-2503"

print(deployment_name)

# %% ../nbs/05a_filter_judges.ipynb #a7b56aa4
path = config.judges_dir / f"{deployment_name}.jsonl"
path.touch()

with open(path, 'r') as f:
    done_claim_ids = [json.loads(line)['claim_id'] for line in f.readlines()]

claim_ids = list(all_claims_dict.keys())  #all_test
experiment_ids = claim_ids.copy()
experiment_ids = [cid for cid in experiment_ids if cid not in done_claim_ids]

# %% ../nbs/05a_filter_judges.ipynb #63310bde
import asyncio

# %% ../nbs/05a_filter_judges.ipynb #5c5f5c21
try: from nbdev.imports import IN_NOTEBOOK
except: IN_NOTEBOOK=False

# %% ../nbs/05a_filter_judges.ipynb #418250b8
async def main():
    claim_id_to_result = await send_to_judge(
        claim_queue=experiment_ids,
        deployment_name=deployment_name,
        batch_size=batch_size,
        sleep_time=sleep_time
    )

if __name__ == "__main__" and not IN_NOTEBOOK:
    asyncio.run(main())

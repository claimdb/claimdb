"""Here, we evaluate different LLM agents on our benchmark. We use propriatery models from Gemini and Anthropic."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/07a_experiments.ipynb.

# %% auto #0
__all__ = ['bird_id_to_example_dict', 'model', 'batch_size', 'sleep_time', 'filename', 'results_path', 'benchmark',
           'extract_json', 'run_result_to_dict_litellm', 'return_coroutines_litellm', 'run_tests_litellm']

# %% ../nbs/07a_experiments.ipynb #64d7be86
import json
from .configuration import *
from .experiments import *
import asyncio
import random
import json
from agents import function_tool, Agent, Runner

# %% ../nbs/07a_experiments.ipynb #b9017069
import re

# %% ../nbs/07a_experiments.ipynb #838a48d7
def extract_json(text: str) -> str:
    match = re.search(r'\{[\s\S]*?\}', text, re.DOTALL)
    return match.group()

# %% ../nbs/07a_experiments.ipynb #7df73122
def run_result_to_dict_litellm(result, ollama=False) -> dict:
    """Convert an Agent's RunResult to a dictionary."""
    info_dict = {}

    if isinstance(result, Exception):
        return {
            'verdict': "",
            'error': str(result),
            'justification': "",
            'model_name': "",
            'model_settings': "",
            'usage': [],
            'to_input_list': []
        }

    # 1. Final output (we will input at the end -- we will del the error if no error)
    info_dict['verdict'] = ""
    info_dict['error'] = ""
    info_dict['justification'] = ""
    info_dict['final_output'] = result.final_output

    # 2. Model Settings
    info_dict['model_name'] = result._last_agent.model
    if ollama: info_dict['model_name'] = info_dict['model_name'].model
    info_dict['model_settings'] = result._last_agent.model_settings.to_json_dict()

    # 3. All Requests Costs (the total is the sum)
    usage = []
    for request_usage in result.context_wrapper.usage.request_usage_entries:
        cached_input_tokens = request_usage.input_tokens_details.cached_tokens
        regular_input_tokens = request_usage.input_tokens - cached_input_tokens
        output_tokens = request_usage.output_tokens

        usage.append(
            {
                "regular_input_tokens": regular_input_tokens,
                "cached_input_tokens": cached_input_tokens,
                "output_tokens": output_tokens,
            }
        )
    info_dict['usage'] = usage

    # 4. The complete Agentic Pipeline
    info_dict['to_input_list'] = result.to_input_list()

    # 5. Extract JSON and parse
    try:
        json_extr = extract_json(result.final_output)
        claim_verdict = ClaimVerdict.model_validate_json(json_extr)
    except Exception as e:
        info_dict['error'] = "JSON Extraction Error."
        return info_dict
    
    del info_dict['error']
    info_dict['verdict'] = claim_verdict.verdict
    info_dict['justification'] = claim_verdict.justification

    return info_dict

# %% ../nbs/07a_experiments.ipynb #bda7cf49
import asyncio
import random

# %% ../nbs/07a_experiments.ipynb #89a22c41
bird_id_to_example_dict = dict()

with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:
    for line in f:
        parsed = json.loads(line)
        bird_id = parsed['bird_id']
        bird_id_to_example_dict[bird_id] = parsed
    
len(bird_id_to_example_dict)

# %% ../nbs/07a_experiments.ipynb #6efbf106
def return_coroutines_litellm(test_claims, model):
    cors = []
    claim_ids = []

    for claim in test_claims:

        tool = tool_cache[claim['db_name']]

        fact_checker_agent = Agent(
            name="Fact-Checker",
            instructions=FACT_CHECKER_PROMPT_3SHOT,
            model=model,
            tools=[tool],
        )

        inp = f"Claim: {claim['claim']}\nExtra Information: {claim['extra_info']}"

        cors.append(Runner.run(fact_checker_agent, inp, max_turns=20))
        
        claim_ids.append(claim['claim_id'])
    
    return cors, claim_ids

# %% ../nbs/07a_experiments.ipynb #3d2adc25
#model = "litellm/gemini/gemini-2.5-flash-lite"
#model = "litellm/anthropic/claude-3-5-haiku-20241022"  # done!
model = "litellm/anthropic/claude-haiku-4-5"
#model = "litellm/anthropic/claude-3-haiku-20240307"  # done!
#model = "litellm/gemini/gemini-3-flash-preview"  # done!
#model = "litellm/gemini/gemini-2.5-flash" # done!

batch_size = 1
sleep_time = 15

filename = model.split("/")[-1]
results_path = config.experiments_dir_pub / f"{filename}.jsonl"
results_path.touch()

# %% ../nbs/07a_experiments.ipynb #2f86ab34
import asyncio

# %% ../nbs/07a_experiments.ipynb #b015e361
with open(results_path, 'r') as f:
    already_tested = [json.loads(line)['claim_id'] for line in f]

benchmark = []
with open(config.final_benchmark_dir / 'test-public.jsonl') as f:
    for line in f: 
        parsed_claim = json.loads(line)
        if parsed_claim['claim_id'] in already_tested: continue
        benchmark.append(parsed_claim)

# %% ../nbs/07a_experiments.ipynb #064bc4c9
import time

# %% ../nbs/07a_experiments.ipynb #79085ba6
async def run_tests_litellm():

    for i in range(0, len(benchmark), batch_size):
        test_claims = benchmark[i:i+batch_size]

        cors, claim_ids = return_coroutines_litellm(test_claims, model)

        results = await asyncio.gather(*cors, return_exceptions=True)

        time.sleep(sleep_time)

        for claim_id, res in zip(claim_ids, results):
            results_dict = {'claim_id': claim_id} | run_result_to_dict_litellm(res)
            results_path.open('a').write(json.dumps(results_dict) + '\n')

# %% ../nbs/07a_experiments.ipynb #70493a7d
try: from nbdev.imports import IN_NOTEBOOK
except: IN_NOTEBOOK=False

# %% ../nbs/07a_experiments.ipynb #9ba7237f
if __name__ == "__main__" and not IN_NOTEBOOK:
    print(f"#Bench Exps: {len(benchmark)}")
    print(model)
    print()
    asyncio.run(run_tests_litellm())

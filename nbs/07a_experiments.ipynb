{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da72c8f",
   "metadata": {},
   "source": [
    "# OpenAI & LiteLLM\n",
    "\n",
    "> Here, we evaluate different LLM agents on our benchmark. We use propriatery models from Gemini and Anthropic.\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiments_litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import json\n",
    "from claimdb.configuration import *\n",
    "from claimdb.experiments import *\n",
    "import asyncio\n",
    "import random\n",
    "import json\n",
    "from agents import function_tool, Agent, Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0351f92b",
   "metadata": {},
   "source": [
    "## LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ed316",
   "metadata": {},
   "source": [
    "### Extract JSON from text\n",
    "\n",
    "Open source models are not good at following structured output instructions. Therefore, we need to extract the JSON part from the text output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9017069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_json(text: str) -> str:\n",
    "    match = re.search(r'\\{[\\s\\S]*?\\}', text, re.DOTALL)\n",
    "    return match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '```json\\n{\\n  \"verdict\": \"ENTAILED\",\\n  \"justification\": \"The database query returned 236 users who created their accounts after 2013 and have more than 10 views, which matches the claim.\"\\n}\\n```'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd54b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_json(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2928cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"```json\\n{\\n  \\\"verdict\\\": \\\"NOT ENOUGH INFO\\\",\\n  \\\"justification\\\": \\\"The database schema for the 'loan' table does not contain information regarding the type of collateral or security for the loans, making it impossible to determine if a loan was secured by a first-lien mortgage on the owner's primary residence.\\\"\\n}\\n```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_json(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClaimVerdict.model_validate_json(extract_json(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213802f5",
   "metadata": {},
   "source": [
    "### Single Example Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"litellm/gemini/gemini-3-flash-preview\"\n",
    "model = \"litellm/gemini/gemini-2.5-flash-lite\"\n",
    "model = \"litellm/anthropic/claude-haiku-4-5\"  # 1/5$\n",
    "model = \"litellm/anthropic/claude-3-haiku-20240307\" \n",
    "model = \"litellm/gemini/gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.final_benchmark_dir / 'test-public.jsonl', \"r\") as f:\n",
    "    all_claims = [json.loads(line) for line in f]\n",
    "\n",
    "claim = all_claims[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "for claim in all_claims:\n",
    "    if \"toxico\" in claim['db_name']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ae163",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = toolbox_client.load_tool(f\"{claim['db_name']}_execute_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd240157",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_checker_agent = Agent(\n",
    "    name=\"Fact-Checker\",\n",
    "    instructions=FACT_CHECKER_PROMPT_3SHOT,\n",
    "    model=model,\n",
    "    tools=[function_tool(tool)],\n",
    "    #output_type=ClaimVerdict\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = f\"Claim: {claim['claim']}\\nExtra Information: {claim['extra_info']}\"\n",
    "#inp = f\"Do you see what tools and metadata of tools you have?\"\n",
    "\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(\n",
    "    fact_checker_agent, \n",
    "    inp, \n",
    "    max_turns=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_json(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClaimVerdict.model_validate_json(extract_json(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373a235",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df73122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def run_result_to_dict_litellm(result, ollama=False) -> dict:\n",
    "    \"\"\"Convert an Agent's RunResult to a dictionary.\"\"\"\n",
    "    info_dict = {}\n",
    "\n",
    "    if isinstance(result, Exception):\n",
    "        return {\n",
    "            'verdict': \"\",\n",
    "            'error': str(result),\n",
    "            'justification': \"\",\n",
    "            'model_name': \"\",\n",
    "            'model_settings': \"\",\n",
    "            'usage': [],\n",
    "            'to_input_list': []\n",
    "        }\n",
    "\n",
    "    # 1. Final output (we will input at the end -- we will del the error if no error)\n",
    "    info_dict['verdict'] = \"\"\n",
    "    info_dict['error'] = \"\"\n",
    "    info_dict['justification'] = \"\"\n",
    "    info_dict['final_output'] = result.final_output\n",
    "\n",
    "    # 2. Model Settings\n",
    "    info_dict['model_name'] = result._last_agent.model\n",
    "    if ollama: info_dict['model_name'] = info_dict['model_name'].model\n",
    "    info_dict['model_settings'] = result._last_agent.model_settings.to_json_dict()\n",
    "\n",
    "    # 3. All Requests Costs (the total is the sum)\n",
    "    usage = []\n",
    "    for request_usage in result.context_wrapper.usage.request_usage_entries:\n",
    "        cached_input_tokens = request_usage.input_tokens_details.cached_tokens\n",
    "        regular_input_tokens = request_usage.input_tokens - cached_input_tokens\n",
    "        output_tokens = request_usage.output_tokens\n",
    "\n",
    "        usage.append(\n",
    "            {\n",
    "                \"regular_input_tokens\": regular_input_tokens,\n",
    "                \"cached_input_tokens\": cached_input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "            }\n",
    "        )\n",
    "    info_dict['usage'] = usage\n",
    "\n",
    "    # 4. The complete Agentic Pipeline\n",
    "    info_dict['to_input_list'] = result.to_input_list()\n",
    "\n",
    "    # 5. Extract JSON and parse\n",
    "    try:\n",
    "        json_extr = extract_json(result.final_output)\n",
    "        claim_verdict = ClaimVerdict.model_validate_json(json_extr)\n",
    "    except Exception as e:\n",
    "        info_dict['error'] = \"JSON Extraction Error.\"\n",
    "        return info_dict\n",
    "    \n",
    "    del info_dict['error']\n",
    "    info_dict['verdict'] = claim_verdict.verdict\n",
    "    info_dict['justification'] = claim_verdict.justification\n",
    "\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe689a",
   "metadata": {},
   "source": [
    "### Run models on All Claims\n",
    "\n",
    "Here, simply change **model** name and run this subsection of the notebook again and again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a22c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "bird_id_to_example_dict = dict()\n",
    "\n",
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        parsed = json.loads(line)\n",
    "        bird_id = parsed['bird_id']\n",
    "        bird_id_to_example_dict[bird_id] = parsed\n",
    "    \n",
    "len(bird_id_to_example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efbf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def return_coroutines_litellm(test_claims, model):\n",
    "    cors = []\n",
    "    claim_ids = []\n",
    "\n",
    "    for claim in test_claims:\n",
    "\n",
    "        tool = tool_cache[claim['db_name']]\n",
    "\n",
    "        fact_checker_agent = Agent(\n",
    "            name=\"Fact-Checker\",\n",
    "            instructions=FACT_CHECKER_PROMPT_3SHOT,\n",
    "            model=model,\n",
    "            tools=[tool],\n",
    "        )\n",
    "\n",
    "        inp = f\"Claim: {claim['claim']}\\nExtra Information: {claim['extra_info']}\"\n",
    "\n",
    "        cors.append(Runner.run(fact_checker_agent, inp, max_turns=20))\n",
    "        \n",
    "        claim_ids.append(claim['claim_id'])\n",
    "    \n",
    "    return cors, claim_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2adc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#model = \"litellm/gemini/gemini-2.5-flash-lite\"\n",
    "#model = \"litellm/anthropic/claude-3-5-haiku-20241022\"  # done!\n",
    "model = \"litellm/anthropic/claude-haiku-4-5\"\n",
    "#model = \"litellm/anthropic/claude-3-haiku-20240307\"  # done!\n",
    "#model = \"litellm/gemini/gemini-3-flash-preview\"  # done!\n",
    "#model = \"litellm/gemini/gemini-2.5-flash\" # done!\n",
    "\n",
    "batch_size = 1\n",
    "sleep_time = 15\n",
    "\n",
    "filename = model.split(\"/\")[-1]\n",
    "results_path = config.experiments_dir_pub / f\"{filename}.jsonl\"\n",
    "results_path.touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "with open(results_path, 'r') as f:\n",
    "    already_tested = [json.loads(line)['claim_id'] for line in f]\n",
    "\n",
    "benchmark = []\n",
    "with open(config.final_benchmark_dir / 'test-public.jsonl') as f:\n",
    "    for line in f: \n",
    "        parsed_claim = json.loads(line)\n",
    "        if parsed_claim['claim_id'] in already_tested: continue\n",
    "        benchmark.append(parsed_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cda92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bc4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79085ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def run_tests_litellm():\n",
    "\n",
    "    for i in range(0, len(benchmark), batch_size):\n",
    "        test_claims = benchmark[i:i+batch_size]\n",
    "\n",
    "        cors, claim_ids = return_coroutines_litellm(test_claims, model)\n",
    "\n",
    "        results = await asyncio.gather(*cors, return_exceptions=True)\n",
    "\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "        for claim_id, res in zip(claim_ids, results):\n",
    "            results_dict = {'claim_id': claim_id} | run_result_to_dict_litellm(res)\n",
    "            results_path.open('a').write(json.dumps(results_dict) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4edcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print()\n",
    "await run_tests_litellm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70493a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "try: from nbdev.imports import IN_NOTEBOOK\n",
    "except: IN_NOTEBOOK=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
    "    print(f\"#Bench Exps: {len(benchmark)}\")\n",
    "    print(model)\n",
    "    print()\n",
    "    asyncio.run(run_tests_litellm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bb329",
   "metadata": {},
   "source": [
    "### No Final Output & JSON Extraction Error (with JSON output)\n",
    "\n",
    "1. `\"final_output\": \"\"` -> This should never really happen and since we cannot rule out `LiteLLM` failing, we delete these cases from the results and run again.\n",
    "\n",
    "2. `\"error\": \"JSON Extraction Error.\"` but the output contains valid JSON-like content -> Here, we can try to extract the JSON ourselves and if successful, we can keep the case. If not, we delete and re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2235024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82790083",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a981c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27871849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dba285",
   "metadata": {},
   "source": [
    "# Vizualise Private Test\n",
    "\n",
    "> Here, we see experiment results!\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcb3379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp visualize_private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import json\n",
    "from claimdb.configuration import *\n",
    "from claimdb.transformation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54518820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.size'] = 14.0\n",
    "plt.rcParams['axes.labelsize'] = 22\n",
    "plt.rcParams['axes.titlesize'] = 22\n",
    "plt.rcParams['xtick.labelsize'] = 20\n",
    "plt.rcParams['ytick.labelsize'] = 20\n",
    "plt.rcParams['legend.fontsize'] = 18\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams.update({'mathtext.default': 'regular' })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751cd1d",
   "metadata": {},
   "source": [
    "## Re-run Bad Cases\n",
    "\n",
    "Here, we will re-run some cases where we suspect the `LiteLLM` framework to have failed as an intermediary. Also, since we have custom logic for structured output extraction (we dont use the official APIs) we need to be extra careful and re-run cases where it is clear that outside factors produced the error and not the LLM (this is rare but can happen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "import json\n",
    "from claimdb.configuration import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbda6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'ministral-3:3b'\n",
    "\n",
    "fix_path = config.experiments_dir_priv / f\"{model}.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9911d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_experiments = []\n",
    "errs = []\n",
    "total = 0\n",
    "\n",
    "seen = []\n",
    "\n",
    "with open(fix_path, 'r') as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "        parsed = json.loads(line)\n",
    "        if parsed['claim_id'] in seen:\n",
    "            print(\"Duplicate:\", parsed['claim_id'])\n",
    "            continue\n",
    "\n",
    "        seen.append(parsed['claim_id'])\n",
    "\n",
    "        #if 'all_messages' in parsed: del parsed['all_messages']\n",
    "\n",
    "        if \"(SQLITE_BUSY)\" in line:\n",
    "            errs.append(parsed)\n",
    "            continue\n",
    "        if '\"An error occurred while running the tool. Please try again. Error: \"' in line:\n",
    "            errs.append(parsed)\n",
    "            continue\n",
    "\n",
    "        if \"error\" in parsed:\n",
    "            if parsed['error'] == \"JSON Extraction Error.\":\n",
    "                errs.append(parsed)\n",
    "                continue\n",
    "            if parsed['error'] == \"Connection error.\":\n",
    "                errs.append(parsed)\n",
    "                continue\n",
    "            if \"status_code: \" in parsed['error']:\n",
    "                errs.append(parsed)\n",
    "                continue\n",
    "            if \"Exceeded maximum retries (20) for output\" in parsed['error']:\n",
    "                errs.append(parsed)\n",
    "                continue\n",
    "            if \"The next tool call(s) would exceed the tool_calls_limit\" in parsed['error']:\n",
    "                errs.append(parsed)\n",
    "                continue\n",
    "            if \"Max turns (20) exceeded\" in parsed['error']:\n",
    "                errs.append(parsed)\n",
    "                continue\n",
    "            if \"Error code: \" in parsed['error']:\n",
    "                errs.append(parsed)\n",
    "                continue\n",
    "            if \"exceeded max retries count of 10\" in parsed['error']:\n",
    "                errs.append(parsed)\n",
    "                continue\n",
    "\n",
    "        correct_experiments.append(parsed)\n",
    "\n",
    "survived = len(correct_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b516abce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1000, Survived: 992, Will Re-Run: 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total: {total}, Survived: {survived}, Will Re-Run: {total - survived}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f7d5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random.shuffle(correct_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6234be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fix_path, 'w') as f:\n",
    "    for entry in correct_experiments:\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1d002",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8a5cb",
   "metadata": {},
   "source": [
    "### High-Level Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "with open(config.final_benchmark_dir / 'test-public.jsonl', \"r\") as f:\n",
    "    pub_test = [json.loads(line) for line in f]\n",
    "    pub_ids = [item['claim_id'] for item in pub_test]\n",
    "\n",
    "with open(config.final_benchmark_dir / 'train.jsonl', \"r\") as f:\n",
    "    train = [json.loads(line) for line in f]\n",
    "\n",
    "# this will fail -- not available publicly\n",
    "with open(config.final_benchmark_dir / 'test-private-with-labels.jsonl', \"r\") as f:\n",
    "    priv_test = [json.loads(line) for line in f]\n",
    "    priv_ids = [item['claim_id'] for item in priv_test]\n",
    "\n",
    "all_claims = pub_test + train + priv_test\n",
    "\n",
    "claim_map = {item['claim_id']: item for item in all_claims}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e3e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aa1ebc5",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c055baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def load_df(model_name, split):\n",
    "    predicted_results = []\n",
    "\n",
    "    ids = []\n",
    "    if split == 'private':\n",
    "        path = config.experiments_dir_priv / f\"{model_name}.jsonl\"\n",
    "        ids = priv_ids\n",
    "    if split == 'public':\n",
    "        path = config.experiments_dir_pub / f\"{model_name}.jsonl\"\n",
    "        ids = pub_ids\n",
    "    \n",
    "    ids = list(ids)\n",
    "\n",
    "    if not os.path.exists(path): return None\n",
    "    \n",
    "    with open(path, \"r\") as f:\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            log = json.loads(line)\n",
    "            claim_id = log['claim_id']\n",
    "            if claim_id not in ids: \n",
    "                continue\n",
    "            else:\n",
    "                ids.remove(claim_id)\n",
    "\n",
    "            verdict = log['verdict']\n",
    "\n",
    "            category = claim_map[claim_id].get('category', None)\n",
    "            ground_truth = claim_map[claim_id]['label']\n",
    "            db_name = claim_map[claim_id]['db_name']\n",
    "\n",
    "            # Tool Calls\n",
    "            num_tool_calls = None\n",
    "            tokens = 0\n",
    "\n",
    "            if \"model_settings\" in log and not \"error\" in log:\n",
    "                num_tool_calls = 0\n",
    "                for item in log['to_input_list']:\n",
    "                    if item.get('type', None) == 'function_call':\n",
    "                        num_tool_calls += 1\n",
    "\n",
    "            if \"model_settings\" in log and 'usage' in log:   # Means we are in OpenAI's SDK\n",
    "                usage_logs = log['usage']\n",
    "                tokens = 0\n",
    "                for usage_dict in usage_logs:\n",
    "                    tokens += sum(usage_dict.values())\n",
    "\n",
    "                #print(f\"{model_name} : {tokens} total tokens\")\n",
    "\n",
    "            if \"model_settings\" not in log and not \"error\" in log:  # Means we are in Pydantic's Agents\n",
    "                if log.get(\"error\", None): \n",
    "                    num_tool_calls = None\n",
    "                    continue\n",
    "                num_tool_calls = 0\n",
    "                for message in log.get('all_messages', []):\n",
    "                    # Check if this is a response message\n",
    "                    if message.get('kind') == 'response':\n",
    "                        # Look through the parts for tool calls\n",
    "                        for part in message.get('parts', []):\n",
    "                            if part.get('part_kind') == 'tool-call':\n",
    "                                num_tool_calls += 1\n",
    "                \n",
    "            if \"model_settings\" not in log: # Means we are in Pydantic's Agents\n",
    "                tokens = 0\n",
    "                for msg in log.get('all_messages', []):\n",
    "                    if 'usage' not in msg:\n",
    "                        continue\n",
    "                    tokens += sum([t for t in msg['usage'].values() if isinstance(t, int)])\n",
    "\n",
    "            # NOTE: gpt-5-nano on private does not have \"model_settings\" but has \"usage\"\n",
    "            if model_name == 'gpt-5-nano':\n",
    "                usage_logs = log['usage']\n",
    "                tokens = 0\n",
    "                for usage_dict in usage_logs:\n",
    "                    tokens += sum(usage_dict.values())\n",
    "                \n",
    "                #print(f\"{model_name} : {tokens} total tokens\")\n",
    "\n",
    "            entry = {\n",
    "                'claim_id': claim_id,\n",
    "                'verdict': verdict,\n",
    "                'ground_truth': ground_truth,\n",
    "                'category': category,\n",
    "                'db_name': db_name,\n",
    "                'tool_calls': num_tool_calls,\n",
    "                'tokens': tokens\n",
    "            }\n",
    "\n",
    "            predicted_results.append(entry)\n",
    "        \n",
    "        for missing_id in ids:\n",
    "            claim = claim_map[missing_id]\n",
    "            entry = {\n",
    "                'claim_id': missing_id,\n",
    "                'verdict': 'MISSING',\n",
    "                'ground_truth': claim['label'],\n",
    "                'category': claim.get('category', None),\n",
    "                'db_name': claim['db_name'],\n",
    "                'tool_calls': None,\n",
    "                'tokens': 0\n",
    "            }\n",
    "            predicted_results.append(entry)\n",
    "    \n",
    "    df = pd.DataFrame(predicted_results)\n",
    "    df['correct'] = df['verdict'] == df['ground_truth']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885de868",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30d5bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "def find_statistics(df):\n",
    "\n",
    "    labels = [\"ENTAILED\", \"CONTRADICTED\", \"NOT ENOUGH INFO\"]\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(df[\"ground_truth\"], df[\"verdict\"])\n",
    "\n",
    "    # Per-class precision / recall / F1\n",
    "    P, R, F1, support = precision_recall_fscore_support(\n",
    "        df[\"ground_truth\"],\n",
    "        df[\"verdict\"],\n",
    "        labels=labels,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        label: {\n",
    "            \"precision\": P[i],\n",
    "            \"recall\": R[i],\n",
    "            \"f1\": F1[i],\n",
    "            \"support\": support[i],\n",
    "        }\n",
    "        for i, label in enumerate(labels)\n",
    "    }\n",
    "\n",
    "    # Macro-F1\n",
    "    macro_f1 = F1.mean()\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        df[\"ground_truth\"],\n",
    "        df[\"verdict\"],\n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "    total_tokens = df['tokens'].sum()\n",
    "\n",
    "    return acc, metrics, macro_f1, cm, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece2576",
   "metadata": {},
   "source": [
    "### Latex Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_millions(n: int | float) -> str:\n",
    "    \"\"\"Format as whole millions (rounded, no decimals).\n",
    "\n",
    "    Examples:\n",
    "      9_300_000   -> '9M'\n",
    "      11_300_000  -> '11M'\n",
    "      12_800_000  -> '13M'\n",
    "      300_000     -> '0M'\n",
    "    \"\"\"\n",
    "    m = int(round(float(n) / 1_000_000))\n",
    "    return f\"{m}M\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3ee1a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_latex_tab(model, acc, metrics, macro_f1, total_tokens, priv=False):\n",
    "    ent = metrics['ENTAILED']\n",
    "    contr = metrics['CONTRADICTED']\n",
    "    nei = metrics['NOT ENOUGH INFO']\n",
    "\n",
    "    if not priv:\n",
    "        print(\n",
    "            f\"\\\\texttt{{{model}}}\\n\"\n",
    "            f\"& \\\\gray{{ {ent['precision']:.3f} }} & \\\\gray{{ {contr['precision']:.3f} }} & \\\\gray{{ {nei['precision']:.3f} }}\\n\"\n",
    "            f\"& \\\\gray{{ {ent['recall']:.3f} }} & \\\\gray{{ {contr['recall']:.3f} }} & \\\\gray{{ {nei['recall']:.3f} }}\\n\"\n",
    "            f\"& {ent['f1']:.3f} & {contr['f1']:.3f} & {nei['f1']:.3f}\\n\"\n",
    "            f\"& {macro_f1:.3f} & {acc:.3f} & {format_millions(total_tokens)} \\\\\\\\\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\\\texttt{{{model}}}\\n\"\n",
    "            f\"& {macro_f1:.3f} & {acc:.3f} & {format_millions(total_tokens)} \\\\\\\\\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c6a09",
   "metadata": {},
   "source": [
    "### Load Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "28fdc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'gpt-4o-mini',\n",
    "    'gpt-4.1-nano',\n",
    "    'gpt-5-nano',\n",
    "    'gpt-5-mini',\n",
    "    'gpt-oss:20b',\n",
    "    'gemini-2.5-flash',\n",
    "    'gemini-3-flash-preview',\n",
    "    'claude-3-haiku-20240307',\n",
    "    'claude-3-5-haiku-20241022',\n",
    "    'claude-haiku-4-5',\n",
    "    'qwen3:1.7b',\n",
    "    'qwen3:4b',\n",
    "    'qwen3:8b',\n",
    "    'qwen3:14b',\n",
    "    'qwen3:32b',\n",
    "    'qwen3-coder:30b',\n",
    "    'ministral-3:3b',\n",
    "    'ministral-3:8b',\n",
    "    'ministral-3:14b',\n",
    "    'mistral-nemo:12b',\n",
    "    'mistral-small:22b',\n",
    "    'magistral:24b',\n",
    "    'devstral:24b',\n",
    "    'devstral-small-2:24b',\n",
    "    'nemotron-3-nano:30b',\n",
    "    'llama3.1:8b',\n",
    "    'llama3.2:3b',\n",
    "    'cogito:14b',\n",
    "    'cogito:32b',\n",
    "    'qwq:32b',\n",
    "]\n",
    "\n",
    "info_dict = {\n",
    "    'gpt-4o-mini': {'name': 'gpt-4o-mini', 'params': None, 'creator': 'OpenAI', 'date_pub': '2025-12-31', 'date_priv': '2026-01-15'},\n",
    "    'gpt-4.1-nano': {'name': 'gpt-4.1-nano', 'params': None, 'creator': 'OpenAI', 'date_pub': '2025-12-30', 'date_priv': '2026-01-11'},\n",
    "    'gpt-5-nano': {'name': 'gpt-5-nano', 'params': None, 'creator': 'OpenAI', 'date_pub': '2026-01-04', 'date_priv': '2026-01-15'},\n",
    "    'gpt-5-mini': {'name': 'gpt-5-mini', 'params': None, 'creator': 'OpenAI', 'date_pub': '2025-12-20', 'date_priv': '2026-01-14'},\n",
    "    'gpt-oss:20b': {'name': 'gpt-oss', 'params': 20, 'creator': 'OpenAI', 'date_pub': '2025-12-20', 'date_priv': '2026-01-14'},\n",
    "    'gemini-2.5-flash': {'name': 'gemini-2.5-flash', 'params': None, 'creator': 'Google', 'date_pub': '2025-12-28', 'date_priv': '2026-01-11'},\n",
    "    'gemini-3-flash-preview': {'name': 'gemini-3-flash', 'params': None, 'creator': 'Google', 'date_pub': '2025-12-26', 'date_priv': '2026-01-14'},\n",
    "    'claude-3-haiku-20240307': {'name': 'claude-3-haiku', 'params': None, 'creator': 'Anthropic', 'date_pub': '2026-01-05', 'date_priv': '2026-01-18'},\n",
    "    'claude-3-5-haiku-20241022': {'name': 'claude-3-5-haiku', 'params': None, 'creator': 'Anthropic', 'date_pub': '2025-12-28', 'date_priv': '2026-01-16'},\n",
    "    'claude-haiku-4-5': {'name': 'claude-haiku-4-5', 'params': None, 'creator': 'Anthropic', 'date_pub': '2025-12-21', 'date_priv': '2026-01-16'},\n",
    "    'qwen3:1.7b': {'name': 'qwen3', 'params': 1.7, 'creator': 'Alibaba', 'date_pub': '2026-01-06', 'date_priv': '2026-01-08'},\n",
    "    'qwen3:4b': {'name': 'qwen3', 'params': 4, 'creator': 'Alibaba', 'date_pub': '2025-12-23', 'date_priv': '2026-01-09'},\n",
    "    'qwen3:8b': {'name': 'qwen3', 'params': 8, 'creator': 'Alibaba', 'date_pub': '2026-01-05', 'date_priv': '2026-01-09'},\n",
    "    'qwen3:14b': {'name': 'qwen3', 'params': 14, 'creator': 'Alibaba', 'date_pub': '2026-01-06', 'date_priv': '2026-01-14'},\n",
    "    'qwen3:32b': {'name': 'qwen3', 'params': 32, 'creator': 'Alibaba', 'date_pub': '2026-01-06', 'date_priv': '2026-01-15'},\n",
    "    'qwen3-coder:30b': {'name': 'qwen3-coder', 'params': 30, 'creator': 'Alibaba', 'date_pub': '2026-01-06', 'date_priv': '2026-01-15'},\n",
    "    'ministral-3:3b': {'name': 'ministral-3', 'params': 3, 'creator': 'Mistral AI', 'date_pub': '2026-01-06', 'date_priv': '2026-01-11'},\n",
    "    'ministral-3:8b': {'name': 'ministral-3', 'params': 8 , 'creator': 'Mistral AI', 'date_pub': '2026-01-06', 'date_priv': '2026-01-15'},\n",
    "    'ministral-3:14b': {'name': 'ministral-3', 'params': 14, 'creator': 'Mistral AI', 'date_pub': '2026-01-01', 'date_priv': '2026-01-17'},\n",
    "    'mistral-small:22b': {'name': 'mistral-small', 'params': 22, 'creator': 'Mistral AI', 'date_pub': '2025-12-20', 'date_priv': '2026-01-19'},\n",
    "    'magistral:24b': {'name': 'magistral', 'params': 24, 'creator': 'Mistral AI', 'date_pub': '2026-01-05', 'date_priv': '2026-01-19'},\n",
    "    'mistral-nemo:12b': {'name': 'mistral-nemo', 'params': 12, 'creator': 'Mistral AI', 'date_pub': '2026-01-05', 'date_priv': '2026-01-18'},\n",
    "    'devstral:24b': {'name': 'devstral', 'params': 24, 'creator': 'Mistral AI', 'date_pub': '2026-01-01', 'date_priv': '2026-01-09'},\n",
    "    'devstral-small-2:24b': {'name': 'devstral-small-2', 'params': 24, 'creator': 'Mistral AI', 'date_pub': '2026-01-04', 'date_priv': '2026-01-14'},\n",
    "    'nemotron-3-nano:30b': {'name': 'nemotron-3-nano', 'params': 30, 'creator': 'NVIDIA', 'date_pub': '2026-01-02', 'date_priv': '2026-01-08'},\n",
    "    'llama3.1:8b': {'name': 'llama3.1', 'params': 8, 'creator': 'Meta', 'date_pub': '2025-12-22', 'date_priv': '2026-01-14'},\n",
    "    'llama3.2:3b': {'name': 'llama3.2', 'params': 3, 'creator': 'Meta', 'date_pub': '2025-12-29', 'date_priv': '2026-01-11'},\n",
    "    'cogito:14b': {'name': 'cogito', 'params': 14, 'creator': 'Deep Cogito', 'date_pub': '2025-12-30', 'date_priv': '2026-01-18'},\n",
    "    'cogito:32b': {'name': 'cogito', 'params': 32, 'creator': 'Deep Cogito', 'date_pub': '2025-12-24', 'date_priv': '2026-01-14'},\n",
    "    'qwq:32b': {'name': 'qwq', 'params': 32, 'creator': 'Alibaba', 'date_pub': '2026-01-05', 'date_priv': '2026-01-17'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "55ea19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5514547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\texttt{gpt-5-nano}\n",
      "& 0.791 & 0.793 & 20M \\\\\n",
      "\n",
      "\\texttt{gpt-5-mini}\n",
      "& 0.828 & 0.828 & 19M \\\\\n",
      "\n",
      "\\texttt{gpt-oss:20b}\n",
      "& 0.759 & 0.763 & 14M \\\\\n",
      "\n",
      "\\texttt{gemini-2.5-flash}\n",
      "& 0.761 & 0.758 & 8M \\\\\n",
      "\n",
      "\\texttt{gemini-3-flash-preview}\n",
      "& 0.807 & 0.805 & 12M \\\\\n",
      "\n",
      "\\texttt{claude-3-5-haiku-20241022}\n",
      "& 0.671 & 0.673 & 11M \\\\\n",
      "\n",
      "\\texttt{claude-haiku-4-5}\n",
      "& 0.799 & 0.792 & 35M \\\\\n",
      "\n",
      "\\texttt{qwen3:1.7b}\n",
      "& 0.231 & 0.363 & 2M \\\\\n",
      "\n",
      "\\texttt{qwen3:4b}\n",
      "& 0.461 & 0.493 & 11M \\\\\n",
      "\n",
      "\\texttt{qwen3:8b}\n",
      "& 0.493 & 0.529 & 10M \\\\\n",
      "\n",
      "\\texttt{qwen3:14b}\n",
      "& 0.465 & 0.506 & 9M \\\\\n",
      "\n",
      "\\texttt{qwen3:32b}\n",
      "& 0.539 & 0.558 & 9M \\\\\n",
      "\n",
      "\\texttt{qwen3-coder:30b}\n",
      "& 0.685 & 0.686 & 22M \\\\\n",
      "\n",
      "\\texttt{ministral-3:3b}\n",
      "& 0.332 & 0.369 & 15M \\\\\n",
      "\n",
      "\\texttt{ministral-3:8b}\n",
      "& 0.543 & 0.556 & 16M \\\\\n",
      "\n",
      "\\texttt{ministral-3:14b}\n",
      "& 0.618 & 0.618 & 23M \\\\\n",
      "\n",
      "\\texttt{mistral-nemo:12b}\n",
      "& 0.345 & 0.354 & 8M \\\\\n",
      "\n",
      "\\texttt{mistral-small:22b}\n",
      "& 0.296 & 0.379 & 15M \\\\\n",
      "\n",
      "\\texttt{magistral:24b}\n",
      "& 0.430 & 0.468 & 18M \\\\\n",
      "\n",
      "\\texttt{devstral:24b}\n",
      "& 0.388 & 0.420 & 0M \\\\\n",
      "\n",
      "\\texttt{devstral-small-2:24b}\n",
      "& 0.572 & 0.576 & 16M \\\\\n",
      "\n",
      "\\texttt{nemotron-3-nano:30b}\n",
      "& 0.660 & 0.656 & 30M \\\\\n",
      "\n",
      "\\texttt{llama3.1:8b}\n",
      "& 0.301 & 0.354 & 7M \\\\\n",
      "\n",
      "\\texttt{llama3.2:3b}\n",
      "& 0.317 & 0.358 & 8M \\\\\n",
      "\n",
      "\\texttt{cogito:14b}\n",
      "& 0.384 & 0.450 & 8M \\\\\n",
      "\n",
      "\\texttt{cogito:32b}\n",
      "& 0.548 & 0.567 & 11M \\\\\n",
      "\n",
      "\\texttt{qwq:32b}\n",
      "& 0.498 & 0.513 & 8M \\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_results = dict()\n",
    "model_dfs = []\n",
    "\n",
    "#for split in ['public', 'private']:\n",
    "#for split in ['public']:\n",
    "for split in ['private']:\n",
    "    for model in models:\n",
    "        df = load_df(model, split)\n",
    "        if df is None:\n",
    "            continue\n",
    "        df['model'] = model\n",
    "        model_dfs.append(df)\n",
    "        acc, metrics, macro_f1, cm, total_tokens = find_statistics(df)\n",
    "        \n",
    "        #print(f\"split: {split}, model: {model}, t: {format_millions(total_tokens)} total tokens: {total_tokens}\")\n",
    "        print_latex_tab(model, acc, metrics, macro_f1, total_tokens, split=='private')\n",
    "        print()\n",
    "\n",
    "df = pd.concat(model_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937c1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c05cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e98c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1c3609b",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9ba429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdde1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claimdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

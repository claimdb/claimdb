{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28375b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee21b4a9",
   "metadata": {},
   "source": [
    "# Transformation of Bird Benchmark\n",
    "\n",
    "> Here, we transform the benchmark into the Fact-Checking benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e03d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "#| hide\n",
    "\n",
    "import json\n",
    "from claimdb.configuration import *\n",
    "from claimdb.preprocess_bird import *\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:\n",
    "    bird_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c782d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a5494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6854"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bird_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48cd6f3",
   "metadata": {},
   "source": [
    "## Contradicted Claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0165dd",
   "metadata": {},
   "source": [
    "### Inputs & Outputs\n",
    "\n",
    "Here, we will define what the LLMs will receive as input and what they should output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0971ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "class ContradictedClaim(BaseModel):\n",
    "    contradicted_claim: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"A contradicted claim.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "class ContradictedClaimCollection(BaseModel):\n",
    "    collection: list[ContradictedClaim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66772968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contradicted_claim': FieldInfo(annotation=str, required=True, description='A contradicted claim.')}\n"
     ]
    }
   ],
   "source": [
    "print(ContradictedClaim.model_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'collection': FieldInfo(annotation=list[ContradictedClaim], required=True)}\n"
     ]
    }
   ],
   "source": [
    "print(ContradictedClaimCollection.model_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d15d64",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf34713",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRADICTED_INSTR = \"\"\"\n",
    "## Role\n",
    "You are a **misleading** spokesperson  **in a controlled evaluation setting**.\n",
    "\n",
    "## Task\n",
    "Given the following inputs:\n",
    "- A question\n",
    "- Its correct answer\n",
    "- The data domain\n",
    "- Optional external knowledge (clarifications)\n",
    "\n",
    "Your task is to produce natural language claims that are factually incompatible with the provided answer. In other words, any reader who knows the correct answer would judge your claim to be false.\n",
    "\n",
    "## Requirements\n",
    "- Each claim must be self-contained and must not use opaque references to earlier context (e.g., \"the answer,\" \"the question,\" \"the earlier claim\", etc.). Instead, any needed context should be stated explicitly within each claim.\n",
    "- Each claim must contradict or be factually incompatible with the answer, directly or indirectly.\n",
    "- Do not restate or explain the external knowledge; assume it is already known to the reader.\n",
    "- Produce between 1 and 3 claims.\n",
    "## Example\n",
    "\n",
    "### Input\n",
    "{\n",
    "  \"question\": \"Which three districts recorded the highest graduation rates in 2022?\",\n",
    "  \"answer\": [\n",
    "    {\n",
    "      \"DistrictName\": \"Redwood Coast Unified\",\n",
    "      \"GradRate\": 0.97\n",
    "    },\n",
    "    {\n",
    "      \"DistrictName\": \"Sierra Vista Union\",\n",
    "      \"GradRate\": 0.96\n",
    "    },\n",
    "    {\n",
    "      \"DistrictName\": \"Mission Creek Unified\",\n",
    "      \"GradRate\": 0.95\n",
    "    }\n",
    "  ],\n",
    "  \"domain\": \"California Schools\",\n",
    "  \"external-knowledge\": \"GradRate = Number of graduates / Total number of eligible seniors\"\n",
    "}\n",
    "\n",
    "### Output\n",
    "Redwood Coast Unified did not lead California's graduation rankings in 2022 — it was Riverbend Joint Unified that posted the top rate.\n",
    "\n",
    "Sierra Vista Union is no longer among the highest graduation-rate districts in 2022.\n",
    "\n",
    "Fairmont Hills Unified surpassed Redwood Coast Unified with 98% of its eligible seniors graduating in 2022, according to data in CA.\n",
    "\"\"\"\n",
    "\n",
    "#conservatively restrict claims to entities already present in the answer or clarifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625ae3e",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa53ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Which active district has the highest average score in Reading?\n",
      "answer: [{'District': 'Palo Alto Unified'}]\n",
      "domain: California Schools\n",
      "external-knowledge: \n"
     ]
    }
   ],
   "source": [
    "inp = format_for_llm(prepare_bird_example(bird_data[11]))\n",
    "\n",
    "for k, v in json.loads(inp).items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": CONTRADICTED_INSTR},\n",
    "        {\"role\": \"user\", \"content\": inp}\n",
    "    ],\n",
    "    text_format=ContradictedClaimCollection,   # ← structured output here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d29d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Which active district has the highest average score in Reading?\n",
      "answer: [{'District': 'Palo Alto Unified'}]\n",
      "domain: California Schools\n",
      "external-knowledge: \n",
      "\n",
      "Generated Contradicted Claims:\n",
      "1. The active district with the highest average Reading score is Cupertino Union, not Palo Alto Unified.\n",
      "\n",
      "2. San Ramon Valley Unified leads all active districts in average Reading performance.\n",
      "\n",
      "3. Palo Alto Unified is not the top performer in Reading; Los Gatos–Saratoga Union High School District holds the highest average score among active districts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "for k, v in json.loads(inp).items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Generated Contradicted Claims:\")\n",
    "for i, item in enumerate(response.output_parsed.collection):\n",
    "    print(f\"{i+1}. {item.contradicted_claim}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536dd58",
   "metadata": {},
   "source": [
    "## Entailed Claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9a58c",
   "metadata": {},
   "source": [
    "### Inputs & Outputs\n",
    "\n",
    "Here, we will define what the LLMs will receive as input and what they should output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461111fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "class EntailedClaim(BaseModel):\n",
    "    entailed_claim: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"An entailed claim.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "class EntailedClaimCollection(BaseModel):\n",
    "    collection: list[EntailedClaim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b22398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailed_claim': FieldInfo(annotation=str, required=True, description='An entailed claim.')}\n"
     ]
    }
   ],
   "source": [
    "print(EntailedClaim.model_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e9dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'collection': FieldInfo(annotation=list[EntailedClaim], required=True)}\n"
     ]
    }
   ],
   "source": [
    "print(EntailedClaimCollection.model_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d7b71",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "ENTAILED_INSTR = \"\"\"\n",
    "## Role\n",
    "You are an **honest** spokesperson **in a controlled evaluation setting**.\n",
    "\n",
    "## Task\n",
    "Given the following inputs:\n",
    "- A question\n",
    "- Its correct answer\n",
    "- The data domain\n",
    "- Optional external knowledge (clarifications)\n",
    "\n",
    "Your task is to produce natural language claims that are consistent with and supported by the provided answer. In other words, any reader who knows the correct answer would judge your claim to be true.\n",
    "\n",
    "## Requirements\n",
    "- Each claim must be self-contained and must not use opaque references to earlier context (e.g., \"the answer,\" \"the question,\" \"the earlier claim\", etc.). Instead, any needed context should be stated explicitly within each claim.\n",
    "- Each claim must follow from or be fully supported by the answer, directly or indirectly.\n",
    "- Do not restate or explain the external knowledge; assume it is already known to the reader.\n",
    "- Produce between 1 and 3 claims.\n",
    "\n",
    "## Example\n",
    "\n",
    "### Input\n",
    "{\n",
    "  \"question\": \"Which three districts recorded the highest graduation rates in 2022?\",\n",
    "  \"answer\": [\n",
    "    {\n",
    "      \"DistrictName\": \"Redwood Coast Unified\",\n",
    "      \"GradRate\": 0.97\n",
    "    },\n",
    "    {\n",
    "      \"DistrictName\": \"Sierra Vista Union\",\n",
    "      \"GradRate\": 0.96\n",
    "    },\n",
    "    {\n",
    "      \"DistrictName\": \"Mission Creek Unified\",\n",
    "      \"GradRate\": 0.95\n",
    "    }\n",
    "  ],\n",
    "  \"domain\": \"California Schools\",\n",
    "  \"external-knowledge\": \"GradRate = Number of graduates / Total number of eligible seniors\"\n",
    "}\n",
    "\n",
    "### Output\n",
    "Redwood Coast Unified led California's graduation rankings in 2022 with a 97% rate.\n",
    "\n",
    "In 2022, California's strongest graduation results came from Redwood Coast Unified, which saw 97% of its eligible seniors finish high school. Sierra Vista Union and Mission Creek Unified followed closely, with graduation rates of 96% and 95%, respectively.\n",
    "\n",
    "Mission Creek Unified achieved a graduation rate of 95% in 2022, placing it among California's top three districts. It ranked just behind Redwood Coast Unified and Sierra Vista Union. The rate indicates the percentage of eligible seniors who graduated.\n",
    "\"\"\"\n",
    "\n",
    "#- Additional claims must differ in factual content, not just wording. Avoid simple paraphrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41c0d9",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0d6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Which active district has the highest average score in Reading?\n",
      "answer: [{'District': 'Palo Alto Unified'}]\n",
      "domain: California Schools\n",
      "external-knowledge: \n"
     ]
    }
   ],
   "source": [
    "inp = format_for_llm(prepare_bird_example(bird_data[11]))\n",
    "\n",
    "for k, v in json.loads(inp).items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": ENTAILED_INSTR},\n",
    "        {\"role\": \"user\", \"content\": inp}\n",
    "    ],\n",
    "    text_format=EntailedClaimCollection,   # ← structured output here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Which active district has the highest average score in Reading?\n",
      "answer: [{'District': 'Palo Alto Unified'}]\n",
      "domain: California Schools\n",
      "external-knowledge: \n",
      "\n",
      "Generated Entailed Claims:\n",
      "1. Among active California school districts, Palo Alto Unified has the highest average Reading score.\n",
      "\n",
      "2. Palo Alto Unified leads all active districts in average Reading performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "for k, v in json.loads(inp).items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Generated Entailed Claims:\")\n",
    "for i, item in enumerate(response.output_parsed.collection):\n",
    "    print(f\"{i+1}. {item.entailed_claim}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58365bb2",
   "metadata": {},
   "source": [
    "## Not Enough Info / Abstain Claims\n",
    "\n",
    "We define abstain as a **refusion to answer dinifitively** (see [FAIR at Meta](https://arxiv.org/pdf/2506.09038)). In our dataset we will create three types of abstaintion:\n",
    "\n",
    "1. **Out-of-Schema Facts**: If a concept has no representation in the schema, then any claim on that concept is unanswerable. No SQL query can decide it.\n",
    "2. **Subjective / Evaluative claims**: If a claim is subjective or evaluative in nature, it cannot be answered definitively by the database (e.g., “dissapointing”, “impressive”, “underwhelming”, “controversial”, “widely criticized” etc.)\n",
    "3. **Counterfactuals / Hypothetical Claims**: If a claim is counterfactual or hypothetical in nature, it cannot be answered definitively by the database (e.g., “If X had happened, would Y be true?”).\n",
    "\n",
    "As you have already guessed, we will also provide the analytical schema to the LLMs so that they can identify out-of-schema facts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038badda",
   "metadata": {},
   "source": [
    "### Inputs & Outputs\n",
    "\n",
    "Here, we will define what the LLMs will receive as input and what they should output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b9ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "class NoInfoClaim(BaseModel):\n",
    "    no_info_claim: str = Field(\n",
    "        ...,\n",
    "        description=\"A NOT ENOUGH INFO claim.\"\n",
    "    )\n",
    "    category: Literal[\"Out-of-Schema\", \"Subjective\", \"Counterfactual\"] = Field(\n",
    "        ...,\n",
    "        description=\"The category of the NOT ENOUGH INFO claim.\"\n",
    "    )\n",
    "\n",
    "class NoInfoClaimCollection(BaseModel):\n",
    "    collection: list[NoInfoClaim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332a4b6",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656812ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "NO_INFO_INSTR = \"\"\"\n",
    "## Role\n",
    "You are a neutral spokespearson **in a controlled evaluation setting**.\n",
    "\n",
    "## Task\n",
    "Given the following inputs:\n",
    "- A question\n",
    "- Its correct answer\n",
    "- The data domain\n",
    "- The schema of the database\n",
    "- Optional external knowledge (clarifications)\n",
    "\n",
    "Your task is to produce natural language claims whose truth **cannot** be determined from the database or the given Q/A. That is, even with full access to both the database and the correct answer, these claims cannot be definitively verified or falsified.\n",
    "\n",
    "## Requirements\n",
    "- Each claim must be self-contained and must not use opaque references to earlier context (e.g., \"the answer,\" \"the question,\" \"the earlier claim\", etc.). Instead, any needed context should be stated explicitly within each claim.\n",
    "- Each claim must *not* be entailed or contradicted by the answer, directly or indirectly.\n",
    "- Each claim must fall into at least one of these categories:\n",
    "  1. **Out-of-schema** — involves concepts the database doesn't store or represent anywhere in its schema.\n",
    "  2. **Subjective/evaluative** — expresses opinions or judgments that cannot be objectively verified.\n",
    "  3. **Counterfactual/hypothetical** — describes an imagined or \"what if\" situation that is not reflected in the actual data.\n",
    "- Produce between 1 and 5 claims.\n",
    "- Do not restate or explain the external knowledge; assume it is already known to the reader.\n",
    "\"\"\"\n",
    "\n",
    "# - Each claim must be self-contained. For example, they must not contain meta-information like \"the answer\", \"the question\", etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2be288",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d58f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Which active district has the highest average score in Reading?\n",
      "answer: [{'District': 'Palo Alto Unified'}]\n",
      "domain: California Schools\n",
      "external-knowledge: \n",
      "db-schema:\n",
      "Table: schools\n",
      "Columns:\n",
      "  - CDSCode\n",
      "  - National Center for Educational Statistics school district identification number\n",
      "  - National Center for Educational Statistics school identification number\n",
      "  - StatusType\n",
      "  - County\n",
      "  - District\n",
      "  - School\n",
      "  - Street\n",
      "  - street address \n",
      "  - City\n",
      "  - Zip\n",
      "  - State\n",
      "  - MailStreet\n",
      "  - mailing street address \n",
      "  - mailing city\n",
      "  - mailing zip \n",
      "  - mailing state\n",
      "  - Phone\n",
      "  - extension\n",
      "  - Website\n",
      "  - OpenDate\n",
      "  - ClosedDate\n",
      "  - Charter\n",
      "  - CharterNum\n",
      "  - FundingType\n",
      "  - District Ownership Code\n",
      "  - The District Ownership Code Type\n",
      "  - School Ownership Code\n",
      "  - School Ownership Code Type\n",
      "  - Education Option Code\n",
      "  - Educational Option Name\n",
      "  - Educational Instruction Level Code\n",
      "  - Educational Instruction Level Name \n",
      "  - grade span offered\n",
      "  - grade span served.\n",
      "  - Virtual\n",
      "  - Magnet\n",
      "  - Latitude\n",
      "  - Longitude\n",
      "  - administrator's first name\n",
      "  - administrator's last name\n",
      "  - administrator's email address\n",
      "  - AdmFName2\n",
      "  - AdmLName2\n",
      "  - AdmEmail2\n",
      "  - AdmFName3\n",
      "  - AdmLName3\n",
      "  - AdmEmail3\n",
      "  - LastUpdate\n",
      "\n",
      "Table: satscores\n",
      "Columns:\n",
      "  - cds\n",
      "  - rtype\n",
      "  - school name\n",
      "  - district name\n",
      "  - county name\n",
      "  - enrollment (1st-12nd grade)\n",
      "  - Number of Test Takers\n",
      "  - average scores in Reading\n",
      "  - average scores in Math\n",
      "  - average scores in writing\n",
      "  - Number of Test Takers Whose Total SAT Scores Are Greater or Equal to 1500\n",
      "\n",
      "Table: frpm\n",
      "Columns:\n",
      "  - CDSCode\n",
      "  - Academic Year\n",
      "  - County Code\n",
      "  - District Code\n",
      "  - School Code \n",
      "  - County Name\n",
      "  - District Name \n",
      "  - School Name\n",
      "  - District Type\n",
      "  - School Type \n",
      "  - Educational Option Type\n",
      "  - NSLP Provision Status\n",
      "  - Charter School (Y/N)\n",
      "  - Charter School Number\n",
      "  - Charter Funding Type\n",
      "  - IRC\n",
      "  - Low Grade\n",
      "  - High Grade\n",
      "  - Enrollment (K-12)\n",
      "  - Free Meal Count (K-12)\n",
      "  - Percent (%) Eligible Free (K-12)\n",
      "  - FRPM Count (K-12)\n",
      "  - Percent (%) Eligible FRPM (K-12)\n",
      "  - Enrollment (Ages 5-17)\n",
      "  - Free Meal Count (Ages 5-17)\n",
      "  -  Percent (%) Eligible Free (Ages 5-17)\n",
      "  - FRPM Count (Ages 5-17)\n",
      "  - Percent (%) Eligible FRPM (Ages 5-17)\n",
      "  - 2013-14 CALPADS Fall 1 Certification Status\n"
     ]
    }
   ],
   "source": [
    "inp = format_for_llm(prepare_bird_example(bird_data[11], with_schema=True))\n",
    "\n",
    "for k, v in json.loads(inp).items():\n",
    "    if k != 'db-schema': \n",
    "        print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(f\"{k}:\")\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fa4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": NO_INFO_INSTR},\n",
    "        {\"role\": \"user\", \"content\": inp}\n",
    "    ],\n",
    "    text_format=NoInfoClaimCollection,   # ← structured output here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad5924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Which active district has the highest average score in Reading?\n",
      "answer: [{'District': 'Palo Alto Unified'}]\n",
      "domain: California Schools\n",
      "external-knowledge: \n",
      "\n",
      "Not-Enough-Info Claims:\n",
      "1. [Out-of-Schema] Palo Alto Unified's average household income is above $200,000.\n",
      "\n",
      "2. [Subjective] Palo Alto Unified's reading curriculum is widely regarded as the most innovative in California.\n",
      "\n",
      "3. [Counterfactual] If the SAT Reading section had used an adaptive, computer-based format instead of a fixed paper test, Palo Alto Unified would not have led the state in average Reading scores.\n",
      "\n",
      "4. [Out-of-Schema] A majority of students in Palo Alto Unified participate in after-school robotics programs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "for k, v in json.loads(inp).items():\n",
    "    if k != 'db-schema':\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Not-Enough-Info Claims:\")\n",
    "for i, item in enumerate(response.output_parsed.collection):\n",
    "    print(f\"{i+1}. [{item.category}] {item.no_info_claim}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962fe91",
   "metadata": {},
   "source": [
    "## Transform BIRD benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ca7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from openai.lib._parsing._responses import type_to_text_format_param, parse_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278cb0e3",
   "metadata": {},
   "source": [
    "### Create Payload Function\n",
    "\n",
    "See [here](https://community.openai.com/t/structured-outputs-with-batch-processing/911076/16) for more information and also [here](https://community.openai.com/t/responses-api-documentation-on-structured-outputs-is-lacking/1356632)\n",
    "\n",
    "We will need to use the `create` API. Thus, we have to specify the output format somehow (reminder: the `.parse` API we have used in the past was doing this for us with `format_output`).\n",
    "\n",
    "Now that we have understood how to:\n",
    "1. specify the output format (i.e., `text = {\"fornmat\": ...}`)\n",
    "2. parse the response (i.e., `parse_response(...)`)\n",
    "\n",
    "we can create a function that will create the payload for each input in our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def construct_payload(request_id: str, # unique (for this batch) request id\n",
    "                      model: str, # model name\n",
    "                      instr: str, # the prompt w/ instructions\n",
    "                      inp: str, # the input\n",
    "                      format_type: Any # the expected output format type (e.g., NoInfoClaimCollection)\n",
    "                      ) -> dict: # returns the payload dict\n",
    "    \"\"\" Construct the payload for the Batch API request. \"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"custom_id\": request_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/responses\",\n",
    "        \"body\": {\n",
    "            \"model\": model,\n",
    "            \"input\": [\n",
    "                {\"role\": \"developer\", \"content\": instr},\n",
    "                {\"role\": \"user\", \"content\": inp}\n",
    "            ],\n",
    "            \"text\": {\"format\": type_to_text_format_param(format_type)}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d92f3",
   "metadata": {},
   "source": [
    "### Create all BIRD payloads\n",
    "\n",
    "There are three payloads for each unique `bird_id`:\n",
    "1. The payload about ENTAILED claims\n",
    "2. The payload about CONTRADICTED claims\n",
    "3. The payload about NOT ENOUGH INFO claims\n",
    "\n",
    "Their difference is the prompt of course (+ the return type)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12772477",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:\n",
    "    bird_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0b3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6854"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bird_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f8855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question_id', 'db_id', 'question', 'evidence', 'SQL', 'difficulty', 'split', 'bird_id', 'result', 'db-schema'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c2169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_data[0]['bird_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161386db",
   "metadata": {},
   "source": [
    "Now we can create all payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33cee9",
   "metadata": {},
   "source": [
    "We give as `custom_id` inside OpenAI's payload system (i.e., `request_id`) the `bird_id`, split (`dev`/`test`) and the claim type (`Entailed`/`Contradicted`/`NoInfo`) so we can identify them later on (when the batch returns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d97e222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6854/6854 [00:02<00:00, 2538.66it/s]\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "payloads = []\n",
    "\n",
    "for example in tqdm.tqdm(bird_data):\n",
    "    regular_inp = format_for_llm(prepare_bird_example(example))\n",
    "    noinfo_inp = format_for_llm(prepare_bird_example(example, with_schema=True))\n",
    "\n",
    "    bird_id = example['bird_id']\n",
    "    split = example['split']\n",
    "    \n",
    "    entailed_payload = construct_payload(\n",
    "        request_id=f\"{bird_id}-{split}-Entailed\",\n",
    "        model=\"gpt-5\",\n",
    "        instr=ENTAILED_INSTR,\n",
    "        inp=regular_inp,\n",
    "        format_type=EntailedClaimCollection\n",
    "    )\n",
    "    payloads.append(entailed_payload)\n",
    "\n",
    "    contradicted_payload = construct_payload(\n",
    "        request_id=f\"{bird_id}-{split}-Contradicted\",\n",
    "        model=\"gpt-5\",\n",
    "        instr=CONTRADICTED_INSTR,\n",
    "        inp=regular_inp,\n",
    "        format_type=ContradictedClaimCollection\n",
    "    )\n",
    "    payloads.append(contradicted_payload)\n",
    "\n",
    "    no_info_payload = construct_payload(\n",
    "        request_id=f\"{bird_id}-{split}-NoInfo\",\n",
    "        model=\"gpt-5\",\n",
    "        instr=NO_INFO_INSTR,\n",
    "        inp=noinfo_inp,\n",
    "        format_type=NoInfoClaimCollection\n",
    "    )\n",
    "    payloads.append(no_info_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eab881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1-dev-Contradicted'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "payloads[4]['custom_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1975b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with open(config.output_data_dir / 'openai_benchmark_payloads.jsonl', 'w') as f:\n",
    "    for payload in payloads:\n",
    "        f.write(json.dumps(payload) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd612866",
   "metadata": {},
   "source": [
    "### Select Payloads to Send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.output_data_dir / 'openai_benchmark_payloads.jsonl', 'r') as f:\n",
    "    payloads = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7a773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0-dev-Entailed'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads[0]['custom_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd339e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_payloads(num_examples: int) -> list[dict]:\n",
    "    \"\"\" Select payloads to send that we have not tested on. \"\"\"\n",
    "\n",
    "    already_tested = set()\n",
    "\n",
    "    tests_done_path = config.output_data_dir / 'openai_raw_results.txt'\n",
    "    if tests_done_path.exists():\n",
    "\n",
    "        with open(tests_done_path, \"r\") as f:\n",
    "            data = f.read()\n",
    "            for line in data.splitlines():\n",
    "                record = json.loads(line)\n",
    "                bird_id, _, label = record['custom_id'].split('-')\n",
    "                already_tested.add((bird_id, label))\n",
    "\n",
    "    i = 0\n",
    "    payloads = []\n",
    "    with open(config.output_data_dir / 'openai_benchmark_payloads.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            payload = json.loads(line)\n",
    "\n",
    "            bird_id, _, label = payload['custom_id'].split('-')\n",
    "            if (bird_id, label) in already_tested:\n",
    "                continue\n",
    "        \n",
    "            payloads.append(payload)\n",
    "            i += 1\n",
    "\n",
    "            if i >= num_examples:\n",
    "                break\n",
    "\n",
    "    return payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for payload in select_payloads(10):\n",
    "    print(payload['custom_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec92e94",
   "metadata": {},
   "source": [
    "### Submit Batches\n",
    "\n",
    "Here, we will use OpenAI's batch API. \n",
    "\n",
    "First, load the payloads and create a file with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a28a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(select_payloads(float('inf')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "payloads = select_payloads(15658)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d124146",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_batch = config.output_data_dir / 'tmp_openai_batch_payloads.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "with open(tmp_batch, 'w') as f:\n",
    "    for payload in payloads:\n",
    "        f.write(json.dumps(payload) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827e221",
   "metadata": {},
   "source": [
    "Now, create a batch on OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2c488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-GrNmFfFScZgSiJr1PR77mj', bytes=57606642, created_at=1765596985, filename='tmp_openai_batch_payloads.jsonl', object='file', purpose='batch', status='processed', expires_at=1768188985, status_details=None)\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(tmp_batch, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(batch_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db76a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-GrNmFfFScZgSiJr1PR77mj'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "batch_input_file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbd316",
   "metadata": {},
   "source": [
    "Once we've successfully uploaded our input file, we can use the input File object's ID to create a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"ALL BIRD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/responses\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": description\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f72a0",
   "metadata": {},
   "source": [
    "We have to find the `batch.id` and save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04787df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'batch_693cdf5698788190a0ed7b548a2fd7be'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "batch_id = batch.id\n",
    "\n",
    "batch_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc24d7",
   "metadata": {},
   "source": [
    "Using the `batch.id` we can now retrieve the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c7740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'completed'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "batch = client.batches.retrieve(batch_id)\n",
    "\n",
    "batch.status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7218eb",
   "metadata": {},
   "source": [
    "### Download Batch Results and Save Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3053c",
   "metadata": {},
   "source": [
    "When the batch is completed, we need to find the `output_file_id` and download the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf97fcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file-BL9zua8D4DJgFsRfnjK6cV'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "output_file_id = batch.output_file_id\n",
    "\n",
    "output_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ebb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "file_response = client.files.content(output_file_id)\n",
    "\n",
    "with open(config.output_data_dir / 'openai_raw_results.txt', \"a\") as f:\n",
    "    f.write(file_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1576ffd",
   "metadata": {},
   "source": [
    "### Parse and Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc630d",
   "metadata": {},
   "source": [
    "#### Example of how to parse\n",
    "\n",
    "We wills start with an example of a single claim response parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005dfe13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0-dev-Entailed'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(config.output_data_dir / 'openai_raw_results.txt', \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "#line = file_response.read().splitlines()[-1]\n",
    "line = data.splitlines()[0]\n",
    "\n",
    "record = json.loads(line)\n",
    "\n",
    "record['custom_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c001b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da5229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0-dev-Entailed'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['custom_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc2354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', 'dev', 'Entailed']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record['custom_id'].split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02937441",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_id, _, label = record['custom_id'].split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691985ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if label == \"Entailed\":\n",
    "    output_format = EntailedClaimCollection\n",
    "if label == \"Contradicted\":\n",
    "    output_format = ContradictedClaimCollection\n",
    "if label == \"NoInfo\":\n",
    "    output_format = NoInfoClaimCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a2cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.EntailedClaimCollection"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57016a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.responses import Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_rec = parse_response(\n",
    "    response=Response.model_validate(record['response']['body']),\n",
    "    text_format=output_format,\n",
    "    input_tools=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c5d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntailedClaim(entailed_claim='The highest eligible free rate for K-12 students among Alameda County schools is 1.0, or 100%.'),\n",
       " EntailedClaim(entailed_claim='At least one Alameda County K-12 school has a Free Meal Count equal to its K-12 Enrollment, producing an eligible free rate of 1.0.')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_rec.output_parsed.collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91618429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Entailed Claims:\n",
      "1. entailed_claim='The highest eligible free rate for K-12 students among Alameda County schools is 1.0, or 100%.'\n",
      "\n",
      "2. entailed_claim='At least one Alameda County K-12 school has a Free Meal Count equal to its K-12 Enrollment, producing an eligible free rate of 1.0.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated {label} Claims:\")\n",
    "for i, item in enumerate(parsed_rec.output_parsed.collection):\n",
    "    print(f\"{i+1}. {item}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e311da",
   "metadata": {},
   "source": [
    "#### Parser Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f45f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from openai.types.responses import Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdf6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def claim_collection_json_to_parsed(claim_collection: dict  # A specific claim collection (e.g., all \"ENTAILED\" claims for the specific BIRD Q/A)\n",
    "                         ):\n",
    "    \"\"\" Parses a claim collection to the OpenAI format with the matched classes.\n",
    "    Each such claim collection is about a specific BIRD Q/A and a specific label (e.g., ENTAILED).\n",
    "    Returns:\n",
    "        bird_id: The BIRD Q/A ID.\n",
    "        label: The label of the claim collection (all claims in the collection are this).\n",
    "        claim_collection_parsed: The parsed claim collection.\n",
    "    \"\"\"\n",
    "    custom_id = claim_collection['custom_id']\n",
    "    bird_id, _, label = custom_id.split(\"-\")\n",
    "\n",
    "    bird_id = int(bird_id)\n",
    "\n",
    "    if label == \"Entailed\":\n",
    "        label = \"ENTAILED\"\n",
    "        output_format = EntailedClaimCollection\n",
    "    if label == \"Contradicted\":\n",
    "        label = \"CONTRADICTED\"\n",
    "        output_format = ContradictedClaimCollection\n",
    "    if label == \"NoInfo\":\n",
    "        label = \"NOT ENOUGH INFO\"\n",
    "        output_format = NoInfoClaimCollection\n",
    "\n",
    "    claim_collection_parsed = parse_response(\n",
    "        response=Response.model_validate(claim_collection['response']['body']),\n",
    "        text_format=output_format,\n",
    "        input_tools=[]\n",
    "    )\n",
    "\n",
    "    return bird_id, label, claim_collection_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc2ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_id, label, parsed = claim_collection_json_to_parsed(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d523d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f955d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENTAILED'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37fd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EntailedClaimCollection(collection=[EntailedClaim(entailed_claim='The highest eligible free rate for K-12 students among Alameda County schools is 1.0, or 100%.'), EntailedClaim(entailed_claim='At least one Alameda County K-12 school has a Free Meal Count equal to its K-12 Enrollment, producing an eligible free rate of 1.0.')])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba22c243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseUsage(input_tokens=648, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=1109, output_tokens_details=OutputTokensDetails(reasoning_tokens=1024), total_tokens=1757)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee40814",
   "metadata": {},
   "source": [
    "#### Parse all and Save\n",
    "\n",
    "**This is where we save our beautiful transformed benchmark!**\n",
    "\n",
    "Each claim will be saved in this format:\n",
    "- `claim_id`: The unique identifier of the claim.\n",
    "- `bird_id`: The unique identifier of the BIRD Q/A pair that this claim is based on.\n",
    "- `claim`: The text of the claim.\n",
    "- `label`: The ground truth label of the claim (`ENTAILED`, `CONTRADICTED`, `NOT ENOUGH INFO`).\n",
    "- OPTIONAL FIELDS:\n",
    "  - `category`: For `Not Enough Info` claims, the category of abstention (`OUT-OF-SCHEMA`, `SUBJECTIVE`, `COUNTERFACTUAL`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with open(config.output_data_dir / 'openai_raw_results.txt', \"r\") as f:\n",
    "    data = [\n",
    "        claim_collection_json_to_parsed(json.loads(line))\n",
    "        for line in f.read().splitlines()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd15b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20562"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28913a29",
   "metadata": {},
   "source": [
    "We also want to keep a dictionary that maps `bird_id` to the unique database name of BIRD. Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11127fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "bird_id_mappings = {}\n",
    "\n",
    "with open(config.bird_dir / 'train_dev.jsonl', 'r') as f:\n",
    "    bird_data = [json.loads(line) for line in f]\n",
    "\n",
    "for item in bird_data:\n",
    "    bird_id_mappings[item['bird_id']] = {\n",
    "        'db_name': item['db_id'],\n",
    "        'extra_info': item['evidence']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb129cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_name': 'california_schools',\n",
       " 'extra_info': 'Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)`'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "bird_id_mappings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b4743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10962"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "len(bird_id_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "claim_id = 0\n",
    "claims = []\n",
    "\n",
    "for bird_id, label, claim_collection in data:\n",
    "    for claim_bucket in claim_collection.output_parsed.collection:\n",
    "        claim_record = {\n",
    "            \"bird_id\": bird_id,\n",
    "            \"claim_id\": claim_id,\n",
    "            \"db_name\": bird_id_mappings[bird_id]['db_name'],\n",
    "            \"claim\": None,\n",
    "            \"extra_info\": bird_id_mappings[bird_id]['extra_info'],\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "        if label == \"ENTAILED\":\n",
    "            claim_record['claim'] = claim_bucket.entailed_claim\n",
    "        if label == \"CONTRADICTED\":\n",
    "            claim_record['claim'] = claim_bucket.contradicted_claim\n",
    "        if label == \"NOT ENOUGH INFO\":\n",
    "            claim_record['claim'] = claim_bucket.no_info_claim\n",
    "            claim_record['category'] = claim_bucket.category.upper()\n",
    "        \n",
    "        claims.append(claim_record)\n",
    "        claim_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21957a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64894"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "len(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335da167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bird_id': 0,\n",
       "  'claim_id': 1,\n",
       "  'db_name': 'california_schools',\n",
       "  'claim': 'At least one Alameda County K-12 school has a Free Meal Count equal to its K-12 Enrollment, producing an eligible free rate of 1.0.',\n",
       "  'extra_info': 'Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)`',\n",
       "  'label': 'ENTAILED'},\n",
       " {'bird_id': 0,\n",
       "  'claim_id': 2,\n",
       "  'db_name': 'california_schools',\n",
       "  'claim': 'No K-12 school in Alameda County has a 100% eligible free rate; the highest share is 0.96.',\n",
       "  'extra_info': 'Eligible free rate for K-12 = `Free Meal Count (K-12)` / `Enrollment (K-12)`',\n",
       "  'label': 'CONTRADICTED'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "claims[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f698f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bird_id': 10670,\n",
       "  'claim_id': 63000,\n",
       "  'db_name': 'movie_3',\n",
       "  'claim': 'BUCKET BROTHERHOOD is the film rented the most times by customers.',\n",
       "  'extra_info': 'film refers to title; film rented the most times refers to title where Max(Count(rental_id))',\n",
       "  'label': 'ENTAILED'},\n",
       " {'bird_id': 10670,\n",
       "  'claim_id': 63001,\n",
       "  'db_name': 'movie_3',\n",
       "  'claim': 'Among all films, BUCKET BROTHERHOOD recorded the highest number of rentals.',\n",
       "  'extra_info': 'film refers to title; film rented the most times refers to title where Max(Count(rental_id))',\n",
       "  'label': 'ENTAILED'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "claims[63000:63002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c8530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird_id': 161,\n",
       " 'claim_id': 1000,\n",
       " 'db_name': 'financial',\n",
       " 'claim': 'The client with ID 13539 owns a junior credit card.',\n",
       " 'extra_info': '',\n",
       " 'label': 'ENTAILED'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "claims[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# it's okay to 'w' because we 'a' on raw results.\n",
    "with open(config.output_data_dir / 'all_claims.jsonl', \"w\") as f:\n",
    "    for claim in claims:\n",
    "        f.write(json.dumps(claim, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aeadc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13c1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03d8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claimdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb5e27",
   "metadata": {},
   "source": [
    "# Preprocess Bird Benchmark\n",
    "\n",
    "> Here, we load, process, and transform the bird benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151971c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp preprocess_bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "import json\n",
    "from claimdb.configuration import *\n",
    "from claimdb.dbops import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340b92b",
   "metadata": {},
   "source": [
    "## Convert all CSV schemas to UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2745c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "def convert_csv_to_utf8(csv_path: Path):\n",
    "    \"\"\"Convert a CSV file to UTF-8 encoding.\"\"\"\n",
    "    # Detect current encoding\n",
    "    with open(csv_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        detected_encoding = result['encoding']\n",
    "\n",
    "    if not detected_encoding:\n",
    "        raise\n",
    "    \n",
    "    # Read with detected encoding, write as UTF-8\n",
    "    try:\n",
    "        content = raw_data.decode(detected_encoding)\n",
    "        # Remove BOM if present\n",
    "        if content.startswith('\\ufeff'):\n",
    "            content = content[1:]\n",
    "        \n",
    "        with open(csv_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        return True, detected_encoding\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Convert all CSV files in BIRD databases\n",
    "converted = []\n",
    "failed = []\n",
    "\n",
    "for db_dir in config.bird_databases_dir.iterdir():\n",
    "    desc_dir = db_dir / 'database_description'\n",
    "    if desc_dir.exists():\n",
    "        for csv_file in desc_dir.glob('*.csv'):\n",
    "            success, info = convert_csv_to_utf8(csv_file)\n",
    "            if success:\n",
    "                converted.append((csv_file.name, info))\n",
    "            else:\n",
    "                failed.append((csv_file.name, info))\n",
    "\n",
    "print(f\"Converted {len(converted)} files to UTF-8\")\n",
    "print(f\"Failed: {len(failed)} files\")\n",
    "\n",
    "if failed:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for name, error in failed:\n",
    "        print(f\"  {name}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "problematic = []\n",
    "for db_dir in config.bird_databases_dir.iterdir():\n",
    "    desc_dir = db_dir / 'database_description'\n",
    "    if desc_dir.exists():\n",
    "        for csv_file in desc_dir.glob('*.csv'):\n",
    "            with open(csv_file, 'rb') as f:\n",
    "                result = chardet.detect(f.read())\n",
    "            # UTF-8-SIG is just UTF-8 with BOM - pandas handles it fine\n",
    "            if result['encoding'] not in ['ascii', 'utf-8', 'UTF-8-SIG']:\n",
    "                problematic.append((csv_file, result))\n",
    "                print(f\"{csv_file.name}: {result['encoding']} ({result['confidence']:.2f})\")\n",
    "\n",
    "print(f\"\\nFound {len(problematic)} problematic files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbab751",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "Here, we load, filter, and merge the bird benchmark dev and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec64abf",
   "metadata": {},
   "source": [
    "### Read & Merge Dev/Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.bird_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b7ef2",
   "metadata": {},
   "source": [
    "Load **dev** and **train** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92632f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dev and train data\n",
    "with open(config.bird_dir / 'dev.json', 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "with open(config.bird_dir / 'train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "dev_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e080f78",
   "metadata": {},
   "source": [
    "Now we will **Annotate and save combined data** in `jsonl` format.:\n",
    "\n",
    "1. Add split label to each entry\n",
    "2. Merge the two sets into one\n",
    "3. Add a unique ID to each entry (of the combined set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "# Add split label to each entry\n",
    "for item in dev_data:\n",
    "    item['split'] = 'dev'\n",
    "\n",
    "for item in train_data:\n",
    "    item['split'] = 'train'\n",
    "\n",
    "# Merge\n",
    "data = dev_data + train_data\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    item['bird_id'] = i\n",
    "\n",
    "# Save merged data\n",
    "with open(config.bird_dir / 'train_dev.jsonl', 'w') as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ee1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSONL\n",
    "with open(config.bird_dir / 'train_dev.jsonl', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc71ea",
   "metadata": {},
   "source": [
    "## Schema Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e26dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def load_bird_table_description(csv_path: Path) -> str:\n",
    "    \"\"\"Load a single table description from BIRD CSV format as compact text.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    table_name = csv_path.stem  # e.g., \"Air Carriers\"\n",
    "    \n",
    "    lines = [f\"Table: {table_name}\"]\n",
    "    lines.append(\"Columns:\")\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        col_name = row['column_name'] if pd.notna(row['column_name']) else row['original_column_name']\n",
    "\n",
    "        # Build compact column line\n",
    "        col_line = f\"  - {col_name}\"\n",
    "        \n",
    "        lines.append(col_line)\n",
    "    \n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def load_bird_database_schema(db_path: Path) -> str:\n",
    "    \"\"\"Load all table descriptions for a BIRD database as compact text.\"\"\"\n",
    "    desc_dir = db_path / 'database_description'\n",
    "    \n",
    "    if not desc_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    tables = []\n",
    "    for csv_file in desc_dir.glob('*.csv'):\n",
    "        table_info = load_bird_table_description(csv_file)\n",
    "        tables.append(table_info)\n",
    "    \n",
    "    return '\\n\\n'.join(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5228e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "# Load schema for a single database\n",
    "db_path = config.bird_databases_dir / 'authors'\n",
    "schema = load_bird_database_schema(db_path)\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aaf666",
   "metadata": {},
   "source": [
    "## Filter out Low-Information Data\n",
    "\n",
    "Now, we will filter the data so that they are **high-information** and also their answer is below 10 rows (parsable by LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSONL\n",
    "with open(config.bird_dir / 'train_dev.jsonl', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4342875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "filtered = []\n",
    "\n",
    "for item in tqdm.tqdm(data):\n",
    "    query = item['SQL']\n",
    "    \n",
    "    if not is_query_high_information(query):\n",
    "        continue\n",
    "\n",
    "    dbdir = config.bird_databases_dir / item['db_id']\n",
    "    dbpath = dbdir / f\"{item['db_id']}.sqlite\"\n",
    "\n",
    "    result = sqlite_execute_with_timeout(dbpath, query)\n",
    "    \n",
    "    if len(result) > 10:\n",
    "        continue\n",
    "\n",
    "    item['result'] = result\n",
    "\n",
    "    item['db-schema'] = load_bird_database_schema(dbdir)\n",
    "\n",
    "    filtered.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf02fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfac33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'w') as f:\n",
    "    for item in filtered:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbf731",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:\n",
    "    filtered = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29049bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9ea14",
   "metadata": {},
   "source": [
    "## LLM-based Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def format_for_llm(d):\n",
    "    \"\"\"Format database schema as JSON for LLMs that prefer structured input.\"\"\"\n",
    "    return json.dumps(d, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def convert_db_name(db_id): # The database ID (e.g., 'world_1')\n",
    "    \"\"\" Converts a database ID to a more human-readable format. \"\"\"\n",
    "    return db_id.replace('_', ' ').title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f462f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def prepare_bird_example(example: dict, with_schema: bool = False):\n",
    "    d = {\n",
    "        'question': example['question'],\n",
    "        'answer': example['result'],\n",
    "        'domain': convert_db_name(example['db_id']),\n",
    "        'external-knowledge': example['evidence']\n",
    "    }\n",
    "\n",
    "    if with_schema:\n",
    "        d['db-schema'] = example['db-schema']\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    prepare_bird_example(filtered[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ce50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    format_for_llm(prepare_bird_example(filtered[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    prepare_bird_example(filtered[0], with_schema=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted = format_for_llm(prepare_bird_example(filtered[0], with_schema=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d16432",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = json.loads(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e43e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a803c",
   "metadata": {},
   "source": [
    "## Filter Out Timeouts + Errors\n",
    "\n",
    "The SQL answer can return `\"error\"` or `\"timeout\"` so we filter this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "from claimdb.configuration import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853bf2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:\n",
    "    bird_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564737ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "problematic_bird_ids = []\n",
    "\n",
    "for ex in bird_data:\n",
    "    if ex['result'] == 'error' or ex['result'] == 'timeout':\n",
    "        problematic_bird_ids.append(ex['bird_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "len(problematic_bird_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:\n",
    "    bird_data = [\n",
    "        json.loads(line) for line in f\n",
    "    ]\n",
    "\n",
    "corrected_bird_data = []\n",
    "for ex in bird_data:\n",
    "    if ex['bird_id'] not in problematic_bird_ids:\n",
    "        corrected_bird_data.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "assert len(corrected_bird_data) == len(bird_data) - len(problematic_bird_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bbac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'w') as f:\n",
    "    for item in corrected_bird_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f1c97",
   "metadata": {},
   "source": [
    "## Filter Out Semantic Mistake Dev Data\n",
    "\n",
    "Following the [paper](https://dl.acm.org/doi/10.1145/3711896.3737427) we filter out 106 question IDs since they are semantically incorrect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cca1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "import json\n",
    "from claimdb.configuration import *\n",
    "from claimdb.dbops import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1390aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "bad_question_ids = [1027, 1029, 519, 523, 530, 23, 70, 72, 584, 1107, 600, 602, 603, 94, 1119, 1120, 1121, 631, 632, 635, 125, 639, 640, 129, 642, 646, 649, 144, 145, 656, 1170, 667, 679, 682, 1197, 686, 687, 1199, 1204, 693, 182, 186, 194, 1219, 709, 710, 1225, 1233, 1243, 221, 1247, 1248, 1256, 1265, 1269, 247, 1273, 1274, 252, 254, 1279, 1284, 271, 1300, 281, 1308, 296, 1322, 812, 309, 341, 342, 343, 855, 349, 360, 1388, 386, 387, 388, 389, 398, 406, 1450, 1454, 431, 1458, 441, 442, 443, 446, 447, 966, 458, 970, 1482, 973, 978, 1491, 986, 993, 484, 1000, 1004, 1530, 1531]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811593f",
   "metadata": {},
   "source": [
    "Convert them to our `bird_id` unique identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd48f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "bad_bird_ids = [70, 94, 129, 144, 182, 186, 254, 309, 342, 349, 386, 446, 458, 523, 603, 631, 632, 639, 640, 642, 656, 667, 679, 682, 687, 693, 709, 710, 966, 970, 973, 986, 1000, 1004, 1027, 1029, 1107, 1119, 1120, 1121, 1170, 1199, 1219, 1243, 1247, 1248, 1256, 1265, 1273, 1274, 1279, 1284, 1300, 1308, 1322, 1388, 1454, 1458, 1482, 1491, 1530, 1531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "bad_bird_ids = []\n",
    "\n",
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        if item['split'] == 'dev' and item['question_id'] in bad_question_ids:\n",
    "            bad_bird_ids.append(item['bird_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c19b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "len(bad_bird_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499eea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "good_bird_data = []\n",
    "\n",
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        if item['bird_id'] not in bad_bird_ids:\n",
    "            good_bird_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31309ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'w') as f:\n",
    "    for item in good_bird_data:\n",
    "        f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd89e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f1671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claimdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

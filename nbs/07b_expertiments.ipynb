{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c73a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee6807",
   "metadata": {},
   "source": [
    "# Pydantic AI / Ollama - Evaluate\n",
    "\n",
    "> Here, we evaluate **open source** models. We have to be careful with structured outputs now. We move to Pydantic AI because of the [tool & output format issue](https://github.com/openai/openai-agents-python/issues/1778). Pydantic AI does not support CGD that is available by Ollama but atleast it enforces output (not perfect) but way better. And we can also use tools with structured output haha.\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891bf91",
   "metadata": {},
   "source": [
    "## Pydantic AI\n",
    "\n",
    "[Github tool & output format issue](https://github.com/openai/openai-agents-python/issues/1778)\n",
    "\n",
    "[Pydantic AI also convert output format to tool](https://github.com/pydantic/pydantic-ai/issues/242)\n",
    "\n",
    "\n",
    "[Force specific tool choice](https://community.openai.com/t/is-it-possible-to-force-agent-to-use-tools/1316526/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1902f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp experiments_pydantic_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object>\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| export\n",
    "\n",
    "import json\n",
    "from claimdb.configuration import config\n",
    "from agents import OpenAIChatCompletionsModel, AsyncOpenAI, Agent, function_tool, Runner\n",
    "from claimdb.experiments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pydantic_ai import Agent, Tool, UsageLimits, ModelRetry\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from pydantic_ai.providers.ollama import OllamaProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db084f",
   "metadata": {},
   "source": [
    "### Test Single Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from claimdb.experiments import _tool_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'gemini-3-flash-preview',\n",
    "    'qwen3:1.7b',\n",
    "    'qwen3:4b',\n",
    "    'qwen3:8b',\n",
    "    'qwen3:14b',\n",
    "    'qwen3:32b',\n",
    "    'qwen3-coder:30b',\n",
    "    'ministral-3:3b',\n",
    "    'ministral-3:8b',\n",
    "    'ministral-3:14b',\n",
    "    'mistral-nemo:12b',\n",
    "    'mistral-small:22b',\n",
    "    'magistral:24b',\n",
    "    'devstral:24b',\n",
    "    'devstral-small-2:24b',\n",
    "    'nemotron-3-nano',\n",
    "    'llama3.1:8b',\n",
    "    'llama3.2:3b',\n",
    "    'cogito:14b',\n",
    "    'cogito:32b',\n",
    "    'qwq:32b',\n",
    "    'gpt-oss:20b',\n",
    "]\n",
    "\n",
    "name = models[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ac910",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = OpenAIChatModel(\n",
    "    model_name=name,\n",
    "    provider=OllamaProvider(base_url='http://localhost:11434/v1'),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.final_benchmark_dir / 'test-public.jsonl', \"r\") as f:\n",
    "    all_claims = [json.loads(line) for line in f]\n",
    "\n",
    "claim = all_claims[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59740c",
   "metadata": {},
   "source": [
    "`retries`: The default number of retries to allow for tool calls and output validation, before raising an error. For model request retries, see the [retries](https://ai.pydantic.dev/api/agent/#pydantic_ai.agent.Agent) documentation.\n",
    "\n",
    "`output_retries`: The maximum number of retries to allow for output validation, defaults to `retries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5878db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "bare_tool = _tool_cache[claim['db_name']]\n",
    "tool_name = tool_cache[claim['db_name']].name\n",
    "tool_description = tool_cache[claim['db_name']].description\n",
    "def safe_tool(sql_query: str):\n",
    "    try:\n",
    "        return bare_tool(sql_query)\n",
    "    except Exception as e:\n",
    "        # do not raise cause we hit `max_retries`\n",
    "        return str(e)\n",
    "tool = Tool(safe_tool, takes_ctx=False, name=tool_name, description=tool_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12337fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    ollama_model,\n",
    "    name=\"Fact-Checker\",\n",
    "    instructions=FACT_CHECKER_PROMPT_3SHOT,\n",
    "    output_type=ClaimVerdict,\n",
    "    output_retries=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = f\"Claim: {claim['claim']}\\nExtra Information: {claim['extra_info']}\"\n",
    "#inp = f\"use the generate final answer tool to answer. What is 20x20\"\n",
    "\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be434dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent.run(inp, usage_limits=UsageLimits(tool_calls_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = json.loads(result.all_messages_json().decode())\n",
    "\n",
    "response = result.output\n",
    "\n",
    "log_dict = dict(result.output) | {'all_messages': all_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a00cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result.all_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c44f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac820c94",
   "metadata": {},
   "source": [
    "### Run All Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "models = [\n",
    "    'gemini-3-flash-preview',\n",
    "    'qwen3:1.7b',\n",
    "    'qwen3:4b',\n",
    "    'qwen3:8b',\n",
    "    'qwen3:14b',\n",
    "    'qwen3:32b',\n",
    "    'qwen3-coder:30b',\n",
    "    'ministral-3:3b',\n",
    "    'ministral-3:8b',\n",
    "    'ministral-3:14b',\n",
    "    'mistral-nemo:12b',\n",
    "    'mistral-small:22b',\n",
    "    'magistral:24b',\n",
    "    'devstral:24b',\n",
    "    'devstral-small-2:24b',\n",
    "    'nemotron-3-nano',\n",
    "    'llama3.1:8b',\n",
    "    'llama3.2:3b',\n",
    "    'cogito:14b',\n",
    "    'cogito:32b',\n",
    "    'qwq:32b',\n",
    "    'gpt-oss:20b',\n",
    "]\n",
    "\n",
    "name = models[-1]\n",
    "\n",
    "results_path = config.experiments_dir_pub / f\"{name}.jsonl\"\n",
    "results_path.touch()\n",
    "\n",
    "port, test_quarters = \"5000\", [1, 2, 3, 4]\n",
    "port, test_quarters = \"5000\", [1]\n",
    "port, test_quarters = \"5001\", [2]\n",
    "port, test_quarters = \"5002\", [3]\n",
    "port, test_quarters = \"5003\", [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e67106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from toolbox_core import ToolboxSyncClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "toolbox_client = ToolboxSyncClient(f\"http://127.0.0.1:{port}\")\n",
    "_tool_cache = dict()\n",
    "tool_cache = dict()\n",
    "\n",
    "for db_name in db_names:\n",
    "    tool = toolbox_client.load_tool(f\"{db_name}_execute_sql\")\n",
    "    _tool_cache[db_name] = tool\n",
    "    tool_cache[db_name] = function_tool(tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "bird_id_to_example_dict = dict()\n",
    "\n",
    "with open(config.bird_dir / 'train_dev_filtered.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        parsed = json.loads(line)\n",
    "        bird_id = parsed['bird_id']\n",
    "        bird_id_to_example_dict[bird_id] = parsed\n",
    "    \n",
    "len(bird_id_to_example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "with open(results_path, 'r') as f:\n",
    "    already_tested = [json.loads(line)['claim_id'] for line in f]\n",
    "\n",
    "benchmark = []\n",
    "with open(config.final_benchmark_dir / 'test-public.jsonl') as f:\n",
    "    for line in f: \n",
    "        parsed_claim = json.loads(line)\n",
    "        if parsed_claim['claim_id'] in already_tested: continue\n",
    "        benchmark.append(parsed_claim)\n",
    "\n",
    "len(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39324b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "n = len(benchmark) // 4\n",
    "q1 = benchmark[0:n]\n",
    "q2 = benchmark[n:2*n]\n",
    "q3 = benchmark[2*n:3*n]\n",
    "q4 = benchmark[3*n:]\n",
    "quarters = [q1, q2, q3, q4]\n",
    "test = sum((quarters[i - 1] for i in test_quarters), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "print(f\"Quarters: {test_quarters}\")\n",
    "print(f\"#Exps Left: {len(test)}\")\n",
    "print(f\"toolbox port: {port}\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3566b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def run_all_pydantic():\n",
    "    for claim in test:\n",
    "        claim_id = claim['claim_id']\n",
    "\n",
    "        bare_tool = _tool_cache[claim['db_name']]\n",
    "        tool_name = tool_cache[claim['db_name']].name\n",
    "        tool_description = tool_cache[claim['db_name']].description\n",
    "        def safe_tool(sql_query: str):\n",
    "            try:\n",
    "                return bare_tool(sql_query)\n",
    "            except Exception as e:\n",
    "                # do not raise cause we hit `max_retries`\n",
    "                return str(e)\n",
    "        tool = Tool(safe_tool, takes_ctx=False, name=tool_name, description=tool_description)\n",
    "\n",
    "        ollama_model = OpenAIChatModel(\n",
    "            model_name=name,\n",
    "            provider=OllamaProvider(base_url='http://localhost:11434/v1'),  \n",
    "        )\n",
    "\n",
    "        agent = Agent(\n",
    "            ollama_model,\n",
    "            name=\"Fact-Checker\",\n",
    "            instructions=FACT_CHECKER_PROMPT_3SHOT,\n",
    "            output_type=ClaimVerdict,\n",
    "            output_retries=20,\n",
    "            tools=[tool]\n",
    "        )\n",
    "\n",
    "\n",
    "        inp = f\"Claim: {claim['claim']}\\nExtra Information: {claim['extra_info']}\"\n",
    "\n",
    "        try:\n",
    "            result = await agent.run(inp, usage_limits=UsageLimits(tool_calls_limit=20))\n",
    "\n",
    "            all_messages = json.loads(result.all_messages_json().decode())\n",
    "            log_dict = dict(result.output) | {'all_messages': all_messages}\n",
    "        except Exception as e:\n",
    "            error = str(e)\n",
    "            log_dict = {'verdict': '', 'justification': '', 'error': error}\n",
    "\n",
    "        results_dict = {'claim_id': claim_id} | log_dict\n",
    "        results_path.open('a').write(json.dumps(results_dict) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_all_pydantic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5182ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "asyncio.run(run_all_pydantic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "toolbox_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1290a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
